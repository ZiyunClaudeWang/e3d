{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/home/alexis/Desktop/E3D', '/home/alexis/Desktop/E3D', '/home/alexis/anaconda3/envs/pytorch3d/lib/python37.zip', '/home/alexis/anaconda3/envs/pytorch3d/lib/python3.7', '/home/alexis/anaconda3/envs/pytorch3d/lib/python3.7/lib-dynload', '', '/home/alexis/anaconda3/envs/pytorch3d/lib/python3.7/site-packages', '/home/alexis/Desktop/pytorch3d', '/home/alexis/anaconda3/envs/pytorch3d/lib/python3.7/site-packages/IPython/extensions', '/home/alexis/.ipython']\n"
     ]
    }
   ],
   "source": [
    "#Imports and dependencies\n",
    "import os\n",
    "from os.path import join\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import math\n",
    "import pandas as pd\n",
    "import imageio\n",
    "from skimage import img_as_ubyte\n",
    "from scipy.ndimage.morphology import binary_dilation\n",
    "from itertools import product\n",
    "from typing import List\n",
    "from tqdm import tqdm_notebook\n",
    "from pytorch3d.io import load_obj, save_obj\n",
    "from pytorch3d.structures import Meshes, Textures\n",
    "from pytorch3d.transforms import Rotate, Translate\n",
    "from pytorch3d.utils import ico_sphere\n",
    "from pytorch3d.ops import sample_points_from_meshes\n",
    "from pytorch3d.renderer import (\n",
    "    SfMPerspectiveCameras, OpenGLPerspectiveCameras, look_at_view_transform, look_at_rotation,\n",
    "    RasterizationSettings, MeshRenderer, MeshRasterizer, BlendParams,\n",
    "    SoftSilhouetteShader, HardPhongShader, PointLights\n",
    ")\n",
    "from dataclasses import dataclass, field, asdict, astuple\n",
    "import numpy as np\n",
    "#Plotting Libs\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "from datetime import datetime\n",
    "import time\n",
    "from copy import deepcopy\n",
    "\n",
    "from plotly.subplots import make_subplots\n",
    "import plotly.graph_objects as go\n",
    "import plotly.express as px\n",
    "\n",
    "from utils.visualization import plot_pointcloud\n",
    "from utils.shapes import Sphere, SphericalSpiral\n",
    "from utils.manager import RenderManager, ImageManager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Matplotlib config nums\n",
    "mpl.rcParams['savefig.dpi'] = 80\n",
    "mpl.rcParams['figure.dpi'] = 80\n",
    "#Set the device\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "if device == \"cuda:0\": torch.cuda.set_device()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2020-06-22 16:29:33--  https://dl.fbaipublicfiles.com/pytorch3d/data/dolphin/dolphin.obj\n",
      "Resolving dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)... 104.22.75.142, 104.22.74.142, 172.67.9.4, ...\n",
      "Connecting to dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)|104.22.75.142|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 156763 (153K) [text/plain]\n",
      "Saving to: ‘data/meshes/dolphin.obj.2’\n",
      "\n",
      "dolphin.obj.2       100%[===================>] 153.09K   600KB/s    in 0.3s    \n",
      "\n",
      "2020-06-22 16:29:34 (600 KB/s) - ‘data/meshes/dolphin.obj.2’ saved [156763/156763]\n",
      "\n",
      "--2020-06-22 16:29:34--  https://dl.fbaipublicfiles.com/pytorch3d/data/teapot/teapot.obj\n",
      "Resolving dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)... 104.22.75.142, 104.22.74.142, 172.67.9.4, ...\n",
      "Connecting to dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)|104.22.75.142|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 152595 (149K) [text/plain]\n",
      "Saving to: ‘data/meshes/teapot.obj.2’\n",
      "\n",
      "teapot.obj.2        100%[===================>] 149.02K   687KB/s    in 0.2s    \n",
      "\n",
      "2020-06-22 16:29:35 (687 KB/s) - ‘data/meshes/teapot.obj.2’ saved [152595/152595]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Download a couple meshes to work with\n",
    "!wget -P data/meshes https://dl.fbaipublicfiles.com/pytorch3d/data/dolphin/dolphin.obj\n",
    "!wget -P data/meshes https://dl.fbaipublicfiles.com/pytorch3d/data/teapot/teapot.obj"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/alexis/Desktop/pytorch3d/pytorch3d/io/obj_io.py:70: UserWarning:\n",
      "\n",
      "Faces have invalid indices\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Load the object without textures and materials\n",
    "verts, faces_idx, _ = load_obj(\"data/meshes/teapot.obj\")\n",
    "faces = faces_idx.verts_idx\n",
    "\n",
    "# Initialize each vertex to be white in color.\n",
    "verts_rgb = torch.ones_like(verts)[None]  # (1, V, 3)\n",
    "textures = Textures(verts_rgb=verts_rgb.to(device))\n",
    "\n",
    "# Create a Meshes object for the teapot. Here we have only one mesh in the batch.\n",
    "teapot_mesh = Meshes(\n",
    "    verts=[verts.to(device)],   \n",
    "    faces=[faces.to(device)], \n",
    "    textures=textures\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a renderer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "cameras = SfMPerspectiveCameras(device=device)\n",
    "\n",
    "\n",
    "# To blend the 100 faces we set a few parameters which control the opacity and the sharpness of \n",
    "# edges. Refer to blending.py for more details. \n",
    "blend_params = BlendParams(sigma=1e-4, gamma=1e-4)\n",
    "\n",
    "# Define the settings for rasterization and shading. Here we set the output image to be of size\n",
    "# 256x256. To form the blended image we use 100 faces for each pixel. We also set bin_size and max_faces_per_bin to None which ensure that \n",
    "# the faster coarse-to-fine rasterization method is used. Refer to rasterize_meshes.py for \n",
    "# explanations of these parameters. Refer to docs/notes/renderer.md for an explanation of \n",
    "# the difference between naive and coarse-to-fine rasterization. \n",
    "raster_settings = RasterizationSettings(\n",
    "    image_size=256, \n",
    "    blur_radius=np.log(1. / 1e-4 - 1.) * blend_params.sigma, \n",
    "    faces_per_pixel=100, \n",
    ")\n",
    "\n",
    "# Create a silhouette mesh renderer by composing a rasterizer and a shader. \n",
    "silhouette_renderer = MeshRenderer(\n",
    "    rasterizer=MeshRasterizer(\n",
    "        cameras=cameras, \n",
    "        raster_settings=raster_settings\n",
    "    ),\n",
    "    shader=SoftSilhouetteShader(blend_params=blend_params)\n",
    ")\n",
    "\n",
    "\n",
    "# We will also create a phong renderer. This is simpler and only needs to render one face per pixel.\n",
    "raster_settings = RasterizationSettings(\n",
    "    image_size=256, \n",
    "    blur_radius=0.0, \n",
    "    faces_per_pixel=100, \n",
    ")\n",
    "# We can add a point light in front of the object. \n",
    "lights = PointLights(device=device, location=((2.0, 2.0, -2.0),))\n",
    "phong_renderer = MeshRenderer(\n",
    "    rasterizer=MeshRasterizer(\n",
    "        cameras=cameras, \n",
    "        raster_settings=raster_settings\n",
    "    ),\n",
    "    shader=HardPhongShader(device=device, lights=lights, cameras=cameras)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ref - look_at_view_transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 800x800 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Select the viewpoint using spherical angles  \n",
    "distance = 3   # distance from camera to the object\n",
    "elevation = 50.0   # angle of elevation in degrees\n",
    "azimuth = 0.0  # No rotation so the camera is positioned on the +Z axis. \n",
    "\n",
    "# Get the position of the camera based on the spherical angles\n",
    "R, T = look_at_view_transform(distance, elevation, azimuth, device=device)\n",
    "\n",
    "# Render the teapot providing the values of R and T. \n",
    "silhouete = silhouette_renderer(meshes_world=teapot_mesh, R=R, T=T)\n",
    "image_ref = phong_renderer(meshes_world=teapot_mesh, R=R, T=T)\n",
    "\n",
    "silhouete = silhouete.cpu().numpy()\n",
    "image_ref = image_ref.cpu().numpy()\n",
    "\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.imshow(silhouete.squeeze()[..., 3])  # only plot the alpha channel of the RGBA image\n",
    "plt.grid(False)\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.imshow(image_ref.squeeze())\n",
    "plt.grid(False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ref - look_at_rotation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "positional argument follows keyword argument (<ipython-input-6-32d1c16c248d>, line 12)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-6-32d1c16c248d>\"\u001b[0;36m, line \u001b[0;32m12\u001b[0m\n\u001b[0;31m    silhouete = silhouette_renderer(meshes_world=teapot_mesh, R=R, T=T, camera)\u001b[0m\n\u001b[0m                                                                       ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m positional argument follows keyword argument\n"
     ]
    }
   ],
   "source": [
    "# Position of the camera in world coordinates\n",
    "x = 2.1\n",
    "y = 3.4\n",
    "z = 0.5\n",
    "cam_posn = torch.from_numpy(np.array([x, y, z], dtype=np.float32))\n",
    "#at - position of the object in world coordinates \n",
    "#up - vector of up direction in world coordinates\n",
    "R = look_at_rotation(cam_posn[None, :], device=device)\n",
    "T = -torch.bmm(R.transpose(1,2), cam_posn[None, :, None])[:, :, 0]\n",
    "\n",
    "# Render the teapot providing the values of R and T. \n",
    "silhouete = silhouette_renderer(meshes_world=teapot_mesh, R=R, T=T, camera)\n",
    "image_ref = phong_renderer(meshes_world=teapot_mesh, R=R, T=T)\n",
    "\n",
    "silhouete = silhouete.cpu().numpy()\n",
    "image_ref = image_ref.cpu().numpy()\n",
    "\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.imshow(silhouete.squeeze()[..., 3])  # only plot the alpha channel of the RGBA image\n",
    "plt.grid(False)\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.imshow(image_ref.squeeze())\n",
    "plt.grid(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Sphere' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-0be2584465e4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m Sphere(\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0mradius\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mnum_points_theta\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mnum_points_phi\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mtheta_min\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpi\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'Sphere' is not defined"
     ]
    }
   ],
   "source": [
    "Sphere(\n",
    "    radius=2,\n",
    "    num_points_theta = 1,\n",
    "    num_points_phi =20,\n",
    "    theta_min = math.pi/2,\n",
    "    theta_max = math.pi/2, \n",
    "    phi_max = math.pi).plot()\n",
    "\n",
    "SphericalSpiral(\n",
    "    c = 6.5, \n",
    "    a = 1.5,\n",
    "    t_min = math.pi,\n",
    "    t_max=2*math.pi,\n",
    "    num_points=1000).plot()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Camera Pose Trajectory and render"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shaded\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "[enforce fail at CPUAllocator.cpp:64] . DefaultCPUAllocator: can't allocate memory: you tried to allocate 209421184 bytes. Error code 12 (Cannot allocate memory)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-93-5d71f6cad8b1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    102\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m \u001b[0mcam_poses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcam_trajectory_rotation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 104\u001b[0;31m \u001b[0mrender\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrender_trajectory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcam_poses\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-93-5d71f6cad8b1>\u001b[0m in \u001b[0;36mrender_trajectory\u001b[0;34m(cam_poses, write_gif)\u001b[0m\n\u001b[1;32m     79\u001b[0m         \u001b[0;31m#sf_cam = SfMPerspectiveCameras(device=device, R=R, T=T)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m         \u001b[0;31m#silhouette = silhouette_renderer(meshes_world=teapot_mesh, R=R[num], T=T[num])\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 81\u001b[0;31m         \u001b[0mimage_ref\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mphong_renderer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmeshes_world\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mteapot_mesh\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mR\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mR\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnum\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mT\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnum\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     82\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m         \u001b[0;31m#silhouette = silhouette.cpu().numpy()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch3d/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    548\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 550\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    551\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    552\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/pytorch3d/pytorch3d/renderer/mesh/renderer.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, meshes_world, **kwargs)\u001b[0m\n\u001b[1;32m     65\u001b[0m                 \u001b[0mpix_to_face\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfragments\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpix_to_face\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m             )\n\u001b[0;32m---> 67\u001b[0;31m         \u001b[0mimages\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfragments\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmeshes_world\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mimages\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch3d/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    548\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 550\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    551\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    552\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/pytorch3d/pytorch3d/renderer/mesh/shader.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, fragments, meshes, **kwargs)\u001b[0m\n\u001b[1;32m     55\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 57\u001b[0;31m         \u001b[0mtexels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minterpolate_vertex_colors\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfragments\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmeshes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     58\u001b[0m         \u001b[0mlights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"lights\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m         \u001b[0mmaterials\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"materials\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmaterials\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/pytorch3d/pytorch3d/renderer/mesh/texturing.py\u001b[0m in \u001b[0;36minterpolate_vertex_colors\u001b[0;34m(fragments, meshes)\u001b[0m\n\u001b[1;32m    110\u001b[0m     \u001b[0mfaces_textures\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvertex_textures\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfaces_packed\u001b[0m\u001b[0;34m]\u001b[0m  \u001b[0;31m# (F, 3, C)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    111\u001b[0m     texels = interpolate_face_attributes(\n\u001b[0;32m--> 112\u001b[0;31m         \u001b[0mfragments\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpix_to_face\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfragments\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbary_coords\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfaces_textures\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    113\u001b[0m     )\n\u001b[1;32m    114\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mtexels\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/pytorch3d/pytorch3d/renderer/mesh/utils.py\u001b[0m in \u001b[0;36minterpolate_face_attributes\u001b[0;34m(pix_to_face, barycentric_coords, face_attributes)\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0mmask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpix_to_face\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[0mpix_to_face\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpix_to_face\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclone\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m     \u001b[0mpix_to_face\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmask\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     65\u001b[0m     \u001b[0midx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpix_to_face\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mN\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mH\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mW\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mK\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpand\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mN\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mH\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mW\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mK\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mD\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m     \u001b[0mpixel_face_vals\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mface_attributes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgather\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mN\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mH\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mW\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mK\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mD\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: [enforce fail at CPUAllocator.cpp:64] . DefaultCPUAllocator: can't allocate memory: you tried to allocate 209421184 bytes. Error code 12 (Cannot allocate memory)\n"
     ]
    }
   ],
   "source": [
    "#Notes: benefit of having varying radiis for trajectory\n",
    "#TODO: Move all the camera stuff to a dataclass\n",
    "\n",
    "#Just to put these somewhere - this should be stored in param file after\n",
    "\n",
    "def cam_trajectory_rotation(num_points: int = 1):\n",
    "    \"\"\"\n",
    "    Returns: list of camera poses (R,T) from trajectory along a spherical spiral\n",
    "    \"\"\"\n",
    "    \n",
    "    #sphere = Sphere(\n",
    "    #    radius=2,\n",
    "    #    num_points_theta = 1,\n",
    "    #    num_points_phi = 30,\n",
    "    #    theta_min = math.pi/2,\n",
    "    #    theta_max = math.pi/2, \n",
    "    #    phi_max = math.pi)\n",
    "    shape = SphericalSpiral(\n",
    "        c = 6.5,\n",
    "        a = 1.5,\n",
    "        t_min = math.pi,\n",
    "        t_max=.0001*math.pi,\n",
    "        num_points=num_points)\n",
    "    up = torch.tensor([[0., 1., 1.]])\n",
    "    R = []\n",
    "    T = []\n",
    "    for cp in shape._tuples:\n",
    "        cp = torch.tensor(cp)\n",
    "        R_new = look_at_rotation(cp[None, :],up=up, device=device)\n",
    "        T_new = -torch.bmm(R_new.transpose(1,2), cp[None, :, None])[:, :, 0]\n",
    "        if not len(R) and not len(T):\n",
    "            R = [R_new]\n",
    "            T = [T_new]\n",
    "        else:\n",
    "            R.append(R_new)\n",
    "            T.append(T_new)\n",
    "    return (R, T)\n",
    "\n",
    "def cam_trajectory_transform(\n",
    "    dist: float = 2.7,\n",
    "    elev_range: list= [10, 50],\n",
    "    azim_range: list= [-180, 180]):\n",
    "    \"\"\"\n",
    "    Input\n",
    "    Returns cameras from ranges of elevations and azimuths\n",
    "    \n",
    "    Not going to use this for now\n",
    "    \"\"\"\n",
    "    \n",
    "    elev = torch.linspace(elev_range[0], elev_range[1], batch_size)\n",
    "    azim = torch.linspace(elev_range[0], elev_range[1], batch_size)\n",
    "    R = []\n",
    "    T = []\n",
    "    for e, a in elev, azim:\n",
    "        R_new, T_new = look_at_view_transform(dist=dist, elev=e, azim=a)\n",
    "        R = torch.stack((R, R_new)) if R else R_new\n",
    "        T = torch.stack((T, T_new)) if T else T_new\n",
    "    return (R, T)\n",
    "    \n",
    "def render_trajectory(cam_poses, write_gif=True):\n",
    "    \n",
    "    \"\"\"\n",
    "    output_folder = \"./data/renders/\"\n",
    "    \n",
    "    silhouette_output = join(output_folder, \"camera_simulation_silhouette.gif\")\n",
    "    silhouette_writer = imageio.get_writer(silhouette_output, mode=\"I\", duration=.2)\n",
    "    \n",
    "    image_output = join(output_folder, \"camera_simulation_image.gif\")\n",
    "    image_writer = imageio.get_writer(image_output, mode=\"I\", duration=.2)\n",
    "    \"\"\"\n",
    "    render_manager = RenderManager(\n",
    "        types=[\"shaded\"],\n",
    "        mesh_name = \"teapot\"\n",
    "    )\n",
    "    # Render the teapot providing the values of R and T.\n",
    "    R, T = cam_poses\n",
    "    for num in range(len(R)):\n",
    "        #gl_cam = OpenGLPerspectiveCameras(device=device, R=R, T=T)\n",
    "        #sf_cam = SfMPerspectiveCameras(device=device, R=R, T=T)\n",
    "        #silhouette = silhouette_renderer(meshes_world=teapot_mesh, R=R[num], T=T[num])\n",
    "        image_ref = phong_renderer(meshes_world=teapot_mesh, R=R[num], T=T[num])\n",
    "        \n",
    "        #silhouette = silhouette.cpu().numpy()\n",
    "        image_ref = image_ref.cpu().numpy()\n",
    "        '''\n",
    "        plt.figure(figsize=(10, 10))\n",
    "        plt.subplot(1, 2, 1)\n",
    "        plt.imshow(silhouette.squeeze()[...,3])  # only plot the alpha channel of the RGBA image\n",
    "        '''\n",
    "        plt.grid(False)\n",
    "        plt.subplot(1, 2, 2)\n",
    "        plt.imshow(image_ref.squeeze())\n",
    "        plt.grid(False)\n",
    "        \n",
    "        render_manager.add_images(\n",
    "            num,\n",
    "            {\"shaded\": image_ref.squeeze()}, \n",
    "            R[num], T[num])\n",
    "        \n",
    "    render_manager.close()\n",
    "    return render_manager\n",
    "    \n",
    "cam_poses = cam_trajectory_rotation()\n",
    "render = render_trajectory(cam_poses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate events from consecutive frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ON = 255\n",
    "OFF = 0\n",
    "threshold = 250\n",
    "\n",
    "def rgb2gray(rgb):\n",
    "    return np.dot(rgb[...,:3], [0.2989, 0.5870, 0.1140])\n",
    "\n",
    "def adaptive_thresholding(diff_frames):\n",
    "    \"\"\"\n",
    "    Experiment: \n",
    "        Determine threshold from mean background illumination\n",
    "    \"\"\"\n",
    "    raise NotImplementedError\n",
    "\n",
    "def dilate_boundaries(diff_frame):\n",
    "    \"\"\"\n",
    "    Experiment:\n",
    "        Dilate the boundaries to introduce more noise on the outside\n",
    "    \"\"\"\n",
    "    raise NotImplementedError\n",
    "\n",
    "def fiter_consts(array_frame):\n",
    "    const_filter = [float(\"nan\"), float(\"inf\")] #Just so we can extend this maybe?\n",
    "    for const in const_filter:\n",
    "        rows, cols = np.where(array_frame == const)\n",
    "\n",
    "filename_output = \"./gen_events.gif\"\n",
    "writer = imageio.get_writer(filename_output, mode='I', duration=.2, loop=0)    \n",
    "\n",
    "for type_key in render.images.keys():\n",
    "    prev_img = None\n",
    "    for num, img in enumerate(render.images[type_key]):\n",
    "        img = deepcopy(img)\n",
    "        img_manager = ImageManager.from_dict(img)        \n",
    "        img_data = img_manager._load\n",
    "        #if type_key == \"shaded\":\n",
    "        #    img_data = rgb2gray(img_data)\n",
    "        #else: continue\n",
    "        if prev_img is not None:\n",
    "            \n",
    "            #if type_key == \"shaded\": print(np.where(img_data!=prev_img))\n",
    "            \n",
    "            \n",
    "            diff_frames = np.subtract(img_data, prev_img)\n",
    "            #if type_key == \"shaded\": \n",
    "                #Plt imshow should only take in a uint8\n",
    "            #    diff_frames = np.uint8(diff_frames)\n",
    "            \n",
    "            #diff_frames[:,:,-1].shape) - only take first two dims of the array\n",
    "            #if type_key == \"shaded\": print(np.where(diff_frames!=0))\n",
    "                \n",
    "            threshold_diff = (diff_frames < threshold) * diff_frames\n",
    "            #threshold_diff = np.where(diff_frames > threshold, 1, 0) #This sets the entire array to 1 or 0\n",
    "            \n",
    "            tanh_diff = np.tanh(threshold_diff).astype(np.uint8)\n",
    "            tanh_diff = np.where(tanh_diff != 0, ON, OFF) #Set the pixels to on or off based off of their value\n",
    "            \n",
    "            #Matplot plotting\n",
    "            plt.figure(figsize=(10, 10))\n",
    "            plt.subplot(1, 2, 1)\n",
    "            plt.imshow(diff_frames) \n",
    "            plt.grid(False)\n",
    "            plt.subplot(1, 2, 2)\n",
    "            plt.imshow(tanh_diff)\n",
    "            plt.grid(False)\n",
    "            \n",
    "            image = img_as_ubyte(tanh_diff)\n",
    "            writer.append_data(image)\n",
    "            \"\"\"\n",
    "            #Plotly plotting - this doesn't work\n",
    "            fig = make_subplots(rows=1, cols=2)\n",
    "            fig.add_trace(\n",
    "                px.imshow(diff_frames),\n",
    "                row = 1, col = 1\n",
    "            )\n",
    "            fig.add_trace(\n",
    "                px.imshow(tanh_diff),\n",
    "                row=1, col=2\n",
    "            )\n",
    "            fig.show()\n",
    "            \"\"\"\n",
    "        prev_img = img_data\n",
    "writer.close()\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageBasedDifferentiableModel(nn.Module):\n",
    "    \n",
    "    def __init__(self, render, renderer, device):\n",
    "        super().__init__()\n",
    "\n",
    "        #Store stuff\n",
    "        self.device = device\n",
    "        self.renderer = renderer\n",
    "        \n",
    "        #Load images from the render\n",
    "        #trajectory = [R, T] where R & T are (N,3)\n",
    "        trajectory = render._trajectory\n",
    "        self.R, self.T = trajectory\n",
    "        #images = np.array([img1, img2]) where img is (256, 256)\n",
    "        self.images_gt = render._images()\n",
    "        self.cameras = SfMPerspectiveCameras(device=self.device, R=self.R, T=self.T)\n",
    "        \n",
    "        #Register to the buffer\n",
    "        #self.register_buffer('trajectory', trafjectory)\n",
    "        #self.register_buffer('images', images)\n",
    "        \n",
    "        #Create a source mesh\n",
    "        template_sphere = ico_sphere(3, device)\n",
    "        verts, faces = template_sphere.get_mesh_verts_faces(0)\n",
    "        #Initialize each vert to have no tetxture\n",
    "        verts_rgb = torch.ones_like(verts)[None]\n",
    "        textures = Textures(verts_rgb=verts_rgb.to(self.device))\n",
    "        self.template_mesh = Meshes(\n",
    "            verts=[verts.to(self.device)],\n",
    "            faces=[faces.to(self.device)],\n",
    "            textures = textures\n",
    "        )\n",
    "        \n",
    "        r = torch.index_select(self.R, 0, torch.tensor([0]))\n",
    "        t = torch.index_select(self.T, 0, torch.tensor([0]))\n",
    "        \n",
    "        render_mesh = self.renderer(meshes_world = self.template_mesh, device = self.device, R=r, T =t)\n",
    "        plt.imshow(render_mesh.cpu().numpy().squeeze())\n",
    "        \n",
    "        self.register_buffer('vertices', self.template_mesh.verts_padded())\n",
    "        self.register_buffer('faces', self.template_mesh.faces_padded())\n",
    "        self.register_buffer('textures', textures.verts_rgb_padded())\n",
    "        \n",
    "        deform_verts = torch.full(self.template_mesh.verts_packed().shape, 0.0, device=device, requires_grad=True)\n",
    "        #Create an optimizable parameter for the mesh\n",
    "        self.register_parameter('deform_verts', nn.Parameter(deform_verts).to(self.device))\n",
    "        \n",
    "    def forward(self):\n",
    "        #Offset the mesh\n",
    "        deformed_mesh_verts = self.template_mesh.offset_verts(self.deform_verts)\n",
    "        texture = Textures(verts_rgb = self.textures)\n",
    "        deformed_mesh = Meshes(verts=deformed_mesh_verts.verts_padded(), faces=deformed_mesh_verts.faces_padded(), textures=texture)\n",
    "        #sample_pts = sample_points_from_meshes(mesh_buffer, 5000)\n",
    "        plot_pointcloud(deformed_mesh)\n",
    "        \n",
    "        r = torch.index_select(self.R, 0, torch.tensor([0]))\n",
    "        print(r, r.shape)\n",
    "        t = torch.index_select(self.T, 0, torch.tensor([0]))\n",
    "        print(t, t.shape)\n",
    "        render_mesh = self.renderer(meshes_world = deformed_mesh, device = self.device, R=r, T =t)\n",
    "        plt.imshow(render_mesh[0,...,:3].detach().cpu().numpy())\n",
    "        \n",
    "        ##BATCH Project 3D mesh to 2D ##\n",
    "        batch_size = len(self.images_gt)\n",
    "        mesh_buffer = deformed_mesh.extend(batch_size)\n",
    "        print(mesh_buffer)\n",
    "        #cameras = OpenGLOrthographicCameras(device=self.device, R=self.R, T=self.T)\n",
    "        projections = self.renderer(mesh_buffer, device=self.device, cameras=self.cameras)\n",
    "        \n",
    "        #Calculate loss of each image\n",
    "        #Other things to try: I\n",
    "        loss = torch.tensor()\n",
    "        for idx in batch_size:\n",
    "            projection = projections[idx][..., 3]\n",
    "            image_gt = self.images_gt[idx]\n",
    "            img_diff = torch.sum((projection - image_gt) ** 2)\n",
    "            loss = torch.cat((loss, img_diff), 0)\n",
    "        loss = torch.mean(loss)\n",
    "        \n",
    "        return loss, projections[0]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "[enforce fail at CPUAllocator.cpp:64] . DefaultCPUAllocator: can't allocate memory: you tried to allocate 208878336 bytes. Error code 12 (Cannot allocate memory)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-97-ebe8abe480a6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# Initialize a model using the renderer, mesh and reference image\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImageBasedDifferentiableModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrender\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mphong_renderer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;31m# Create an optimizer. Here we are using Adam and we pass in the parameters of the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-96-81dafd5692ff>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, render, renderer, device)\u001b[0m\n\u001b[1;32m     35\u001b[0m         \u001b[0mt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex_select\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m         \u001b[0mrender_mesh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrenderer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmeshes_world\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtemplate_mesh\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mR\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mT\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m         \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrender_mesh\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch3d/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    548\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 550\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    551\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    552\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/pytorch3d/pytorch3d/renderer/mesh/renderer.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, meshes_world, **kwargs)\u001b[0m\n\u001b[1;32m     65\u001b[0m                 \u001b[0mpix_to_face\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfragments\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpix_to_face\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m             )\n\u001b[0;32m---> 67\u001b[0;31m         \u001b[0mimages\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfragments\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmeshes_world\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mimages\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch3d/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    548\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 550\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    551\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    552\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/pytorch3d/pytorch3d/renderer/mesh/shader.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, fragments, meshes, **kwargs)\u001b[0m\n\u001b[1;32m     55\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 57\u001b[0;31m         \u001b[0mtexels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minterpolate_vertex_colors\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfragments\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmeshes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     58\u001b[0m         \u001b[0mlights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"lights\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m         \u001b[0mmaterials\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"materials\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmaterials\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/pytorch3d/pytorch3d/renderer/mesh/texturing.py\u001b[0m in \u001b[0;36minterpolate_vertex_colors\u001b[0;34m(fragments, meshes)\u001b[0m\n\u001b[1;32m    110\u001b[0m     \u001b[0mfaces_textures\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvertex_textures\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfaces_packed\u001b[0m\u001b[0;34m]\u001b[0m  \u001b[0;31m# (F, 3, C)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    111\u001b[0m     texels = interpolate_face_attributes(\n\u001b[0;32m--> 112\u001b[0;31m         \u001b[0mfragments\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpix_to_face\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfragments\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbary_coords\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfaces_textures\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    113\u001b[0m     )\n\u001b[1;32m    114\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mtexels\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/pytorch3d/pytorch3d/renderer/mesh/utils.py\u001b[0m in \u001b[0;36minterpolate_face_attributes\u001b[0;34m(pix_to_face, barycentric_coords, face_attributes)\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0mmask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpix_to_face\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[0mpix_to_face\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpix_to_face\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclone\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m     \u001b[0mpix_to_face\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmask\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     65\u001b[0m     \u001b[0midx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpix_to_face\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mN\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mH\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mW\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mK\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpand\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mN\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mH\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mW\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mK\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mD\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m     \u001b[0mpixel_face_vals\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mface_attributes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgather\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mN\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mH\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mW\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mK\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mD\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: [enforce fail at CPUAllocator.cpp:64] . DefaultCPUAllocator: can't allocate memory: you tried to allocate 208878336 bytes. Error code 12 (Cannot allocate memory)\n"
     ]
    }
   ],
   "source": [
    "# We will save images periodically and compose them into a GIF.\n",
    "filename_output = \"./projection_loss.gif\"\n",
    "writer = imageio.get_writer(filename_output, mode='I', duration=0.3)\n",
    "\n",
    "# Initialize a model using the renderer, mesh and reference image\n",
    "model = ImageBasedDifferentiableModel(render, phong_renderer, device).to(device)\n",
    "\n",
    "# Create an optimizer. Here we are using Adam and we pass in the parameters of the model\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.05) #Hyperparameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loop = tqdm_notebook(range(200))\n",
    "for i in loop:\n",
    "    optimizer.zero_grad()\n",
    "    loss, _ = model()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    loop.set_description('Optimizing (loss %.4f)' % loss.data)\n",
    "    \n",
    "    if loss.item() < 200:\n",
    "        break\n",
    "    \n",
    "    # Save outputs to create a GIF. \n",
    "    if i % 10 == 0:\n",
    "        R = look_at_rotation(model.camera_position[None, :], device=model.device)\n",
    "        T = -torch.bmm(R.transpose(1, 2), model.camera_position[None, :, None])[:, :, 0]   # (1, 3)\n",
    "        image = phong_renderer(meshes_world=model.meshes.clone(), R=R, T=T)\n",
    "        image = image[0, ..., :3].detach().squeeze().cpu().numpy()\n",
    "        image = img_as_ubyte(image)\n",
    "        writer.append_data(image)\n",
    "        \n",
    "        plt.figure()\n",
    "        plt.imshow(image[..., :3])\n",
    "        plt.title(\"iter: %d, loss: %0.2f\" % (i, loss.data))\n",
    "        plt.grid(\"off\")\n",
    "        plt.axis(\"off\")\n",
    "    \n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Pytorch3D",
   "language": "python",
   "name": "pytorch3d"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
