{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Imports and dependencies\n",
    "import os\n",
    "from os.path import join, abspath, dirname\n",
    "import sys\n",
    "sys.path.insert(0, abspath(join(\"..\", dirname(os.getcwd()))))\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import math\n",
    "import pandas as pd\n",
    "import random\n",
    "import imageio\n",
    "from skimage import img_as_ubyte\n",
    "from scipy.ndimage.morphology import binary_dilation\n",
    "from itertools import product\n",
    "from typing import List\n",
    "from tqdm import tqdm_notebook\n",
    "from pytorch3d.io import load_obj, save_obj\n",
    "from pytorch3d.structures import Meshes\n",
    "from pytorch3d.transforms import Rotate, Translate\n",
    "from pytorch3d.utils import ico_sphere\n",
    "from pytorch3d.ops import sample_points_from_meshes\n",
    "from pytorch3d.renderer import (\n",
    "    SfMPerspectiveCameras, OpenGLPerspectiveCameras, look_at_view_transform, look_at_rotation,\n",
    "    RasterizationSettings, MeshRenderer, MeshRasterizer, BlendParams,\n",
    "    SoftSilhouetteShader, HardPhongShader, PointLights, TexturesVertex, HardFlatShader\n",
    ")\n",
    "from pytorch3d.loss import (\n",
    "    mesh_laplacian_smoothing, \n",
    "    mesh_normal_consistency\n",
    ")\n",
    "from torchvision import transforms\n",
    "\n",
    "from dataclasses import dataclass, field, asdict, astuple\n",
    "import numpy as np\n",
    "#Plotting Libs\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "from datetime import datetime\n",
    "import time\n",
    "from copy import deepcopy\n",
    "\n",
    "from plotly.subplots import make_subplots\n",
    "import plotly.graph_objects as go\n",
    "import plotly.express as px\n",
    "\n",
    "import json\n",
    "\n",
    "from utils.visualization import plot_pointcloud\n",
    "from utils.shapes import Sphere, SphericalSpiral\n",
    "from utils.manager import RenderManager, ImageManager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Matplotlib config nums\n",
    "mpl.rcParams['savefig.dpi'] = 90\n",
    "mpl.rcParams['figure.dpi'] = 90\n",
    "#Set the device\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "if device == \"cuda:0\": torch.cuda.set_device()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a Renderer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_size = (64, 64)\n",
    "\n",
    "cameras = SfMPerspectiveCameras(device=device)\n",
    "\n",
    "# To blend the 100 faces we set a few parameters which control the opacity and the sharpness of \n",
    "# edges. Refer to blending.py for more details. \n",
    "blend_params = BlendParams(sigma=1e-4, gamma=1e-4)\n",
    "\n",
    "# Define the settings for rasterization and shading. Here we set the output image to be of size\n",
    "# 256x256. To form the blended image we use 100 faces for each pixel. We also set bin_size and max_faces_per_bin to None which ensure that \n",
    "# the faster coarse-to-fine rasterization method is used. Refer to rasterize_meshes.py for \n",
    "# explanations of these parameters. Refer to docs/notes/renderer.md for an explanation of \n",
    "# the difference between naive and coarse-to-fine rasterization. \n",
    "raster_settings = RasterizationSettings(\n",
    "    image_size=img_size[0], \n",
    "    blur_radius=np.log(1. / 1e-4 - 1.) * blend_params.sigma, \n",
    "    faces_per_pixel=100, \n",
    ")\n",
    "\n",
    "# Create a silhouette mesh renderer by composing a rasterizer and a shader. \n",
    "silhouette_renderer = MeshRenderer(\n",
    "    rasterizer=MeshRasterizer(\n",
    "        cameras=cameras, \n",
    "        raster_settings=raster_settings\n",
    "    ),\n",
    "    shader=SoftSilhouetteShader(blend_params=blend_params)\n",
    ")\n",
    "\n",
    "\n",
    "# We will also create a phong renderer. This is simpler and only needs to render one face per pixel.\n",
    "raster_settings = RasterizationSettings(\n",
    "    image_size=img_size[0], \n",
    "    blur_radius=1e-5, \n",
    "    faces_per_pixel=1, \n",
    ")\n",
    "# We can add a point light in front of the object. \n",
    "lights = PointLights(\n",
    "    device=device, \n",
    "    location=[[3.0, 3.0, 0.0]], \n",
    "    diffuse_color=((1.0, 1.0, 1.0),),\n",
    "    specular_color=((1.0, 1.0, 1.0),),\n",
    ")\n",
    "phong_renderer = MeshRenderer(\n",
    "    rasterizer=MeshRasterizer(\n",
    "        cameras=cameras, \n",
    "        raster_settings=raster_settings\n",
    "    ),\n",
    "    shader=HardFlatShader(device=device, lights=lights, cameras=cameras)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Event Renderer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ON = 254\n",
    "OFF = 0\n",
    "threshold = 254.5/255\n",
    "\n",
    "def gray(img):\n",
    "    return np.dot(img[...,:3], [0.2989, 0.5870, 0.1140])\n",
    "\n",
    "def event_renderer(img1, img2, render_type):\n",
    "    \n",
    "    if render_type == \"phong\":\n",
    "        img1 = gray(img1)\n",
    "        img2 = gray(img2)\n",
    "        \n",
    "    diff_frames = img2 - img1\n",
    "    \n",
    "    threshold_diff = (diff_frames < threshold) * diff_frames\n",
    "\n",
    "    tanh_diff = np.tanh(threshold_diff)\n",
    "    #tanh_diff = np.where(tanh_diff != 0, OFF, ON)\n",
    "    \n",
    "    return tanh_diff"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a trajectory and Render"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Diff Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def neg_iou_loss(predict, target):\n",
    "    dims = tuple(range(predict.ndimension())[1:])\n",
    "    intersect = (predict * target).sum(dims)\n",
    "    union = (predict + target - predict * target).sum(dims) + 1e-6\n",
    "    return 1. - (intersect / union).sum() / intersect.nelement()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MeshDeformationModel(nn.Module):\n",
    "    \n",
    "    def __init__(self, device, template_mesh = None):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.device = device\n",
    "        \n",
    "        #Create a source mesh\n",
    "        if not template_mesh:\n",
    "            template_mesh = ico_sphere(2, device)\n",
    "        \n",
    "        verts, faces = template_mesh.get_mesh_verts_faces(0)\n",
    "        #Initialize each vert to have no tetxture\n",
    "        verts_rgb = torch.ones_like(verts)[None]\n",
    "        textures = TexturesVertex(verts_rgb.to(self.device))\n",
    "        self.template_mesh = Meshes(\n",
    "            verts=[verts.to(self.device)],\n",
    "            faces=[faces.to(self.device)],\n",
    "            textures = textures\n",
    "        )\n",
    "        \n",
    "        self.register_buffer('vertices', self.template_mesh.verts_padded() * 1.3)\n",
    "        self.register_buffer('faces', self.template_mesh.faces_padded())\n",
    "        self.register_buffer('textures', textures.verts_features_padded())\n",
    "        \n",
    "        deform_verts = torch.zeros_like(self.template_mesh.verts_packed(), device=device, requires_grad=True)\n",
    "        #deform_verts = torch.full(self.template_mesh.verts_packed().shape, 0.0, device=device, requires_grad=True)\n",
    "        #Create an optimizable parameter for the mesh\n",
    "        self.register_parameter('deform_verts', nn.Parameter(deform_verts).to(self.device))\n",
    "        \n",
    "        laplacian_loss = mesh_laplacian_smoothing(template_mesh, method=\"uniform\")\n",
    "        flatten_loss = mesh_normal_consistency(template_mesh)\n",
    "        \n",
    "    def forward(self, batch_size):\n",
    "        #Offset the mesh\n",
    "        deformed_mesh_verts = self.template_mesh.offset_verts(self.deform_verts)\n",
    "        texture = TexturesVertex(self.textures)\n",
    "        deformed_mesh = Meshes(verts=deformed_mesh_verts.verts_padded(), faces=deformed_mesh_verts.faces_padded(), textures=texture)\n",
    "        deformed_meshes = deformed_mesh.extend(batch_size)\n",
    "    \n",
    "        laplacian_loss = mesh_laplacian_smoothing(deformed_mesh, method=\"uniform\")\n",
    "        flatten_loss = mesh_normal_consistency(deformed_mesh)\n",
    "        \n",
    "        return deformed_meshes, laplacian_loss, flatten_loss\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IMAGE-BASED\n",
    "### Initialize the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_tensor(t, batch_size, indices = None):\n",
    "    l = t.shape[0]\n",
    "    if l < batch_size:\n",
    "        return\n",
    "    if not indices:\n",
    "        step = int(l / batch_size)\n",
    "        start = random.randint(0, step - 1)\n",
    "        indices = list(range(start, l, step))\n",
    "    return t[indices], indices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Experiments indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create path to tests folder\n",
    "tests_path = \"../data/tests/dolphin-sep23-baseline\"\n",
    "batch_size = int(360/ 8) #=45\n",
    "ratios = [.25, .50, .75, .100]\n",
    "#Fetch indices\n",
    "with open(join(tests_path, \"indices.json\"), 'r') as f:\n",
    "    indices = json.load(f)\n",
    "#Generate all the good indices and create sets of experiments\n",
    "experiments = {}\n",
    "good_indices = set(range(360)) - set([elem for ind in indices.values() for elem in ind])\n",
    "for name, ind in indices.items():\n",
    "    if name == \"outliers\":\n",
    "        continue\n",
    "    for ratio in ratios:\n",
    "        num_clean = int((1 - ratio) * batch_size)\n",
    "        num_noisy = batch_size - num_clean\n",
    "        \n",
    "        ind_clean = sorted(random.choices(list(good_indices), k = num_clean))\n",
    "        ind_noisy = sorted(random.choices(ind, k = num_noisy))\n",
    "        \n",
    "        experiment_indices = ind_clean + ind_noisy\n",
    "        \n",
    "        experiments[f\"{name}_{int(100*ratio)}-{100-int(100*ratio)}\"] = experiment_indices\n",
    "\n",
    "experiments = {}\n",
    "experiments[\"best_1\"] = sorted(random.choices(list(good_indices), k = batch_size))\n",
    "experiments[\"best_2\"] = sorted(random.choices(list(good_indices), k = batch_size))\n",
    "experiments[\"best_3\"] = sorted(random.choices(list(good_indices), k = batch_size))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create path to tests folder\n",
    "for experiment_name, indices in experiments.items():\n",
    "    \n",
    "    path = join(tests_path, experiment_name)\n",
    "    os.makedirs(path, exist_ok=True)\n",
    "\n",
    "    weight_silhouette = 1\n",
    "    weight_laplacian = .1\n",
    "    weight_flatten = .001\n",
    "\n",
    "    #Create a loss plotting object\n",
    "    #loss_ax = plot_loss(num_losses = 3)\n",
    "\n",
    "    # Initialize a model using the renderer, template mesh and reference image\n",
    "    model = MeshDeformationModel(device).to(device)\n",
    "\n",
    "    # Create an optimizer. Here we are using Adam and we pass in the parameters of the model\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=.001, betas=(0.5, 0.99)) #Hyperparameter tuning\n",
    "\n",
    "\n",
    "    #render = RenderManager.from_directory(dir_num=60)\n",
    "    render = RenderManager.from_path(\"../data/renders/test_dolphin/002-dolphin_2020-09-23T11:45:50/\")\n",
    "    R, T = render._trajectory\n",
    "    R, indices = sample_tensor(R, batch_size)\n",
    "    T, _ = sample_tensor(T, batch_size, indices)\n",
    "\n",
    "    images_gt = render._images(type_key=\"silhouette_pred\", img_size = img_size).to(device)\n",
    "    images_gt, _ = sample_tensor(images_gt, batch_size, indices)\n",
    "\n",
    "    cameras = SfMPerspectiveCameras(device=device, R=R, T=T)\n",
    "\n",
    "    results = {}\n",
    "    results[\"indices\"] = indices\n",
    "\n",
    "    # We will save images periodically and compose them into a GIF.\n",
    "    filename_output = join(path, \"projection_loss.gif\")\n",
    "    writer = imageio.get_writer(filename_output, mode='I', duration=0.1)\n",
    "\n",
    "    loop = tqdm_notebook(range(2000))\n",
    "    laplacian_losses = []\n",
    "    flatten_losses = []\n",
    "    silhouette_losses = []\n",
    "\n",
    "    for i in loop:\n",
    "\n",
    "        mesh, laplacian_loss, flatten_loss = model(batch_size)\n",
    "\n",
    "        images_pred = silhouette_renderer(mesh.clone(), device=device, cameras=cameras)\n",
    "\n",
    "        silhouette_loss = neg_iou_loss(images_gt, images_pred[...,-1])\n",
    "        #ssd_loss = torch.sum((images_gt - images_pred[...,-1]) ** 2).mean()\n",
    "\n",
    "        loss = silhouette_loss * weight_silhouette + laplacian_loss * weight_laplacian + flatten_loss * weight_flatten\n",
    "\n",
    "        loop.set_description('Optimizing (loss %.4f)' % loss.data)\n",
    "\n",
    "        silhouette_losses.append(silhouette_loss * weight_silhouette)\n",
    "        laplacian_losses.append(laplacian_loss * weight_laplacian)\n",
    "        flatten_losses.append(flatten_loss * weight_flatten)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if i % 100 == 0:\n",
    "            #Write images\n",
    "            image = images_pred.detach().cpu().numpy()[0][...,-1]\n",
    "\n",
    "            writer.append_data((255*image).astype(np.uint8))\n",
    "            #imageio.imsave(join(path, f\"mesh_{i}.png\"), (255*image).astype(np.uint8))\n",
    "\n",
    "            f, (ax1, ax2) = plt.subplots(1, 2)\n",
    "\n",
    "            image = img_as_ubyte(image)\n",
    "            ax1.imshow(image)\n",
    "            ax1.set_title(\"Deformed Mesh\")\n",
    "\n",
    "            ax2.plot(silhouette_losses, label=\"Silhouette Loss\")\n",
    "            ax2.plot(laplacian_losses, label=\"Laplacian Loss\")\n",
    "            ax2.plot(flatten_losses, label=\"Flatten Loss\")\n",
    "            ax2.legend(fontsize=\"16\")\n",
    "            ax2.set_xlabel(\"Iteration\", fontsize=\"16\")\n",
    "            ax2.set_ylabel(\"Loss\", fontsize=\"16\")\n",
    "            ax2.set_title(\"Loss vs iterations\", fontsize=\"16\")\n",
    "\n",
    "            plt.show()\n",
    "\n",
    "    #Save obj, gif, individual losses, mesh similarity metric\n",
    "    verts, faces = mesh.get_mesh_verts_faces(0)\n",
    "    save_obj(join(path, \"mesh.obj\"), verts, faces)\n",
    "\n",
    "    def plot_and_save(elems: list, name: str):\n",
    "        plt.plot(elems, label=f\"{name} Loss\")\n",
    "        plt.legend(fontsize=\"16\")\n",
    "        plt.xlabel(\"Iteration\")\n",
    "        plt.ylabel(\"Loss\")\n",
    "        plt.title(\"Loss vs iterations\")\n",
    "        plt.savefig(join(path, f\"{name}_loss.png\"))\n",
    "\n",
    "    plot_and_save(silhouette_losses, \"silhouette\")\n",
    "    plot_and_save(laplacian_losses, \"laplacian\")\n",
    "    plot_and_save(flatten_losses, \"flatten\")\n",
    "\n",
    "    results[\"silhouette_loss\"] = [s.detach().cpu().numpy().tolist() for s in silhouette_losses]\n",
    "    results[\"laplacian_loss\"] = [l.detach().cpu().numpy().tolist() for l in laplacian_losses]\n",
    "    results[\"flatten_loss\"] = [f.detach().cpu().numpy().tolist() for f in flatten_losses]\n",
    "    with open(join(path, \"results.json\"), 'w') as f:\n",
    "        json.dump(results, f)\n",
    "\n",
    "    writer.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch3d",
   "language": "python",
   "name": "pytorch3d"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
