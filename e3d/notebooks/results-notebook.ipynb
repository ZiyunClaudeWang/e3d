{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Imports and dependencies\n",
    "import os\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0,1,2,3,4\"\n",
    "\n",
    "from os.path import join, abspath, dirname\n",
    "import sys\n",
    "sys.path.insert(0, abspath(join(\"..\", dirname(os.getcwd()))))\n",
    "         \n",
    "from collections import defaultdict\n",
    "import re\n",
    "import random\n",
    "import json\n",
    "import torch\n",
    "import imageio\n",
    "from PIL import Image, ImageOps\n",
    "from tqdm import tqdm_notebook\n",
    "from skimage import img_as_ubyte\n",
    "from pytorch3d.structures import Meshes\n",
    "from pytorch3d.utils import ico_sphere\n",
    "from pytorch3d.io import load_obj, save_obj\n",
    "from pytorch3d.datasets import (\n",
    "    ShapeNetCore,\n",
    "    collate_batched_meshes\n",
    ")\n",
    "from pytorch3d.loss import (\n",
    "    mesh_laplacian_smoothing, \n",
    "    mesh_normal_consistency\n",
    ")\n",
    "from pytorch3d.renderer import (\n",
    "    PerspectiveCameras, RasterizationSettings, MeshRenderer, MeshRasterizer, \n",
    "    BlendParams, SoftSilhouetteShader, SoftPhongShader, DirectionalLights, TexturesVertex, \n",
    "    TexturesAtlas, HardPhongShader, HardFlatShader, look_at_view_transform, PointLights\n",
    ")\n",
    "from pytorch3d.io import load_objs_as_meshes\n",
    "\n",
    "from dataclasses import dataclass, field, asdict, astuple\n",
    "\n",
    "import numpy as np\n",
    "#Plotting Libs\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "\n",
    "\n",
    "from synth_dataset.trajectory import cam_trajectory\n",
    "from synth_dataset.mesh import (\n",
    "    load_meshes, mesh_random_translation, rotate_mesh_around_axis, \n",
    "    translate_mesh_on_axis, scale_mesh\n",
    ")\n",
    "from synth_dataset.event_renderer import generate_event_frames\n",
    "\n",
    "from utils.visualization import plot_trajectory_cameras\n",
    "from utils.manager import RenderManager, ImageManager\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mpl.rcParams['savefig.dpi'] = 150\n",
    "mpl.rcParams['figure.dpi'] = 150\n",
    "#Set the device\n",
    "device = torch.device(\"cuda:1\" if torch.cuda.is_available() else \"cpu\")\n",
    "torch.cuda.set_device(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "managers = [46, 8]\n",
    "trajectory_path = \"data/renders/{0}_{1}_shapenet\".format(\"test\", \"car\")\n",
    "managers = [RenderManager.from_directory(m, trajectory_path) for m in managers]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mesh_names = [\"car-46\", \"car-8\"]\n",
    "path = \"../data/video-mesh-results\"\n",
    "for m in mesh_names:\n",
    "    dir_path = os.listdir(join(path, m))[0]\n",
    "    m_path = join(path, m, dir_path)\n",
    "    mesh_stuff = torch.load(m_path)\n",
    "    verts = mesh_stuff['vertices'].unsqueeze(0)\n",
    "    faces = mesh_stuff['faces'].unsqueeze(0)\n",
    "    mesh = Meshes(verts=verts, faces=faces)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cameras = PerspectiveCameras(device=device)\n",
    "\n",
    "# We will also create a phong renderer. This is simpler and only needs to render one face per pixel.\n",
    "raster_settings = RasterizationSettings(\n",
    "    image_size=280, \n",
    "    blur_radius=0, \n",
    "    faces_per_pixel=100,\n",
    "    max_faces_per_bin=500000\n",
    ")\n",
    "# We can add a point light in front of the object. \n",
    "#lights = PointLights(device=device, location=((2., 2.0, 2.0),))\n",
    "lights = PointLights(\n",
    "    device=device, \n",
    "    location=[[-3, 4, -3], ], \n",
    "    diffuse_color=((.4, .4, .4),),\n",
    "    specular_color=((.4, .4, .4),),\n",
    ")\n",
    "\n",
    "flat_renderer = MeshRenderer(\n",
    "    rasterizer=MeshRasterizer(\n",
    "        cameras=cameras, \n",
    "        raster_settings=raster_settings\n",
    "    ),\n",
    "    shader=HardFlatShader(device=device, lights=lights, cameras=cameras)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cars = ['19']\n",
    "manager_path = \"data/renders/{0}_{1}_shapenet\".format(\"test\", \"chair\")\n",
    "#pmo_path = \"../../../photometric-mesh-optim/optimized_mesh/0/\"\n",
    "pmo_path = \"../data/video-mesh-results\"\n",
    "for c in cars: \n",
    "    #Collect the manager\n",
    "    manager = RenderManager.from_directory(int(c), manager_path)\n",
    "    R, T = manager._trajectory\n",
    "    R, T = R.to(device), T.to(device)\n",
    "    #Collect the mesh\n",
    "    dir_path = os.listdir(join(pmo_path, f\"chair_{c}\"))[0]\n",
    "    m_path = join(pmo_path, f\"chair_{c}\", dir_path)\n",
    "    mesh_stuff = torch.load(m_path)\n",
    "    verts = mesh_stuff['vertices']\n",
    "    faces = mesh_stuff['faces']\n",
    "    center = verts.mean(0)\n",
    "    verts = verts - center\n",
    "    scale = max(verts.abs().max(0)[0])\n",
    "    verts = verts / scale\n",
    "    verts_rgb = torch.ones_like(verts)\n",
    "    textures = TexturesVertex(verts_features=[verts_rgb.to(device)])\n",
    "    mesh = Meshes(verts=[verts], faces=[faces], textures=textures)\n",
    "    mesh = mesh.to(device)\n",
    "    mesh = rotate_mesh_around_axis(mesh, [90,90,180], flat_renderer,dist=2, device=device)\n",
    "    #break\n",
    "    #Create the cameras for rendering\n",
    "    images = []\n",
    "    for i in range(R.shape[0]):\n",
    "        img_gt = np.array(manager.get_image('phong', i, (224, 224))) / 255\n",
    "        print(img_gt.shape)\n",
    "        plt.imshow(img_gt)\n",
    "        plt.show()\n",
    "        image = torch.clamp(flat_renderer(mesh, R=R[[i]], T=T[[i]] * 2, device=device), 0, 1)\n",
    "        image *= 255\n",
    "        img_np = image.detach().cpu()[...,:3].squeeze().numpy().astype(np.uint8)\n",
    "        plt.imshow(img_np)\n",
    "        plt.show()\n",
    "        img_pil = Image.fromarray(img_np)\n",
    "        \n",
    "        save_path = join(pmo_path, f\"chair_{c}\",'images', f\"{i}.png\")\n",
    "        img_pil.save(save_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Re-Rendering obj's"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_obj =\"../data/renders/test2_dolphin/001-dolphin_2020-10-29T00:28:24/predicted_meshGT-Pose.obj\"\n",
    "#path_to_obj = \"../data/renders/test_chair_shapenet/019-chair_2020-11-08T01:41:02/predicted_meshGT-Pose.obj\"\n",
    "verts, faces, _ = load_obj(path_to_obj)\n",
    "verts_rgb = torch.ones_like(verts)\n",
    "textures = TexturesVertex(verts_features=[verts_rgb.to(device)])\n",
    "mesh = Meshes([verts], [faces.verts_idx], textures)\n",
    "meshes = mesh.to(device).extend(6)\n",
    "elev = [30] * 6\n",
    "azim = torch.linspace(110, 310, 6)\n",
    "dist = 2\n",
    "R, T = look_at_view_transform(elev=elev, azim=azim, dist=dist, device=device)\n",
    "images = torch.clamp(flat_renderer(meshes, R=R, T=T, device=device), 0, 1)\n",
    "images*=255\n",
    "images_np = images.detach().cpu()[...,:3].squeeze().numpy().astype(np.uint8)\n",
    "for count, img in enumerate(images_np):\n",
    "    if count in [1,2]:\n",
    "        img_pil = Image.fromarray(img)\n",
    "        img_pil.save(f'tmp{count}.png')\n",
    "    plt.imshow(img)\n",
    "    plt.show()\n",
    "    \n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aggregate Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import statistics\n",
    "path_to_test = \"../data/renders/test_car_shapenet\"\n",
    "agg_results = {\n",
    "    'q_mean_error': [],\n",
    "    't_mean_error': [],\n",
    "    'q_mean_error_trans': [],\n",
    "    't_mean_error_trans': [],\n",
    "    'seg_iou': [],\n",
    "    'GT-Pose': {\n",
    "        'mesh_iou': [],\n",
    "        'chamfer_loss': [],\n",
    "    },\n",
    "    'Rot+Trans': {\n",
    "        'mesh_iou': [],\n",
    "        'chamfer_loss': [],\n",
    "    },\n",
    "    'Trans+LookAt': {\n",
    "        'mesh_iou': [],\n",
    "        'chamfer_loss': [],\n",
    "    }\n",
    "}\n",
    "#Collect\n",
    "for f in os.listdir(path_to_test):\n",
    "    f_path = join(path_to_test, f)\n",
    "    path_to_json = join(f_path, 'reconstruction_results.json')\n",
    "    if not os.path.exists(path_to_json):\n",
    "        continue\n",
    "    with open(path_to_json, \"r\") as f:\n",
    "        results = json.load(f)\n",
    "    for k in agg_results.keys():\n",
    "        if isinstance(agg_results[k], list):\n",
    "            value = results.get(k, 0)\n",
    "            if isinstance(value, list): value = value[0]\n",
    "            agg_results[k].append(value)\n",
    "        elif isinstance(agg_results[k], dict) and k in results:\n",
    "            for sub_k in agg_results[k].keys():\n",
    "                value = results[k].get(sub_k, 0)\n",
    "                #if value > 30:\n",
    "                    #print('whoooooo', value)\n",
    "                    #continue\n",
    "                agg_results[k][sub_k].append(value)\n",
    "\n",
    "#print(agg_results['q_mean_error'])  \n",
    "print(len(agg_results['q_mean_error']))\n",
    "acc = 0\n",
    "for idx in range(len(agg_results['q_mean_error'])):\n",
    "    if agg_results['q_mean_error'][idx] < 45 and agg_results['t_mean_error'][idx] < 1:\n",
    "        acc += 1\n",
    "#Aggregate\n",
    "for k in agg_results.keys():\n",
    "    if isinstance(agg_results[k], list):\n",
    "        if k.find('_error') != -1:\n",
    "        \n",
    "            agg_results[k] = statistics.median(sorted(agg_results[k])[:110])           \n",
    "                \n",
    "        else:\n",
    "            agg_results[k] = sum(agg_results[k]) / len(agg_results[k])\n",
    "    elif isinstance(agg_results[k], dict):\n",
    "        for sub_k in agg_results[k].keys():\n",
    "            if len(agg_results[k][sub_k]) == 0: continue\n",
    "            agg_results[k][sub_k] = sum(agg_results[k][sub_k]) / len(agg_results[k][sub_k])\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chair = {'q_mean_error': 54.866579805867055,\n",
    " 't_mean_error': 0.5388392225075304,\n",
    " 'q_mean_error_trans': 61.90197501721671,\n",
    " 't_mean_error_trans': 0.19155412594254098,\n",
    " 'seg_iou': 0.474987036726448,\n",
    " 'GT-Pose': {'mesh_iou': 0.3589355796575546,\n",
    "  'chamfer_loss': 2.796546784894807},\n",
    " 'Rot+Trans': {'mesh_iou': 0.7112318029006323,\n",
    "  'chamfer_loss': 25.072881937026978},\n",
    " 'Trans+LookAt': {'mesh_iou': 0.4695173714842115,\n",
    "  'chamfer_loss': 3.812564043771653}}\n",
    "\n",
    "car = {'q_mean_error': 50.346605033409304,\n",
    " 't_mean_error': 0.5105624865831399,\n",
    " 'q_mean_error_trans': 56.54089143101711,\n",
    " 't_mean_error_trans': 0.1783685179927,\n",
    " 'seg_iou': 0.35914203379212356,\n",
    " 'GT-Pose': {'mesh_iou': [], 'chamfer_loss': []},\n",
    " 'Rot+Trans': {'mesh_iou': 0.603664345211453,\n",
    "  'chamfer_loss': 20.83055844004192},\n",
    " 'Trans+LookAt': {'mesh_iou': 0.43306054671605426,\n",
    "  'chamfer_loss': 2.560623920153058}}\n",
    "\n",
    "car2 = {'q_mean_error': 38.32957077026367,\n",
    " 't_mean_error': 0.4174906313419342,\n",
    " 'q_mean_error_trans': 49.968566052849695,\n",
    " 't_mean_error_trans': 0.4174906313419342,\n",
    " 'seg_iou': 0.34866725090073375,\n",
    " 'GT-Pose': {'mesh_iou': 0.27524283102580477,\n",
    "  'chamfer_loss': 1.6745700046183571},\n",
    " 'Rot+Trans': {'mesh_iou': [], 'chamfer_loss': []},\n",
    " 'Trans+LookAt': {'mesh_iou': [], 'chamfer_loss': []}}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Event Contours"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ev_path = \"../data/renders/test_chair_shapenet/019-chair_2020-11-08T01:41:02/events/12_event.png\"\n",
    "ev_path_2 = \"../data/renders/test_car_shapenet/017-car_2020-11-08T01:36:43/events/17_event.png\"\n",
    "img_path = \"../data/renders/test_chair_shapenet/019-chair_2020-11-08T01:41:02/phong/10_phong.png\"\n",
    "ev = np.array(Image.open(ev_path))\n",
    "ev2 = np.array(Image.open(ev_path_2))\n",
    "ev *=4\n",
    "img = np.array(Image.open(img_path))\n",
    "image_white = np.all(ev==[0,0,0], axis=-1)\n",
    "image_other = np.all(ev!=[0,0,0], axis=-1)\n",
    "ev[image_white] = img[image_white] \n",
    "#ev[image_other] *= 2\n",
    "plt.imshow(ev)\n",
    "plt.show()\n",
    "pil_img = Image.fromarray(ev)\n",
    "pil_img.save('test.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from synth_dataset.trajectory import cam_trajectory\n",
    "from utils.visualization import plot_trajectory_cameras\n",
    "\n",
    "pepper = ['elev', 'dist']\n",
    "variation = ['dist', 'elev', 'azim']\n",
    "random_start = [ 'azim', 'dist']\n",
    "R, T = cam_trajectory(variation, pepper, random_start, 22)\n",
    "cameras = PerspectiveCameras(R=R, T=T, device=device)\n",
    "#fig = plot_trajectory_cameras(cameras, device)\n",
    "mesh_p = \"../data/meshes/teapot.obj\"\n",
    "verts, faces, _ = load_obj(mesh_p)\n",
    "verts_rgb = torch.ones_like(verts)\n",
    "textures = TexturesVertex(verts_features=[verts_rgb.to(device)])\n",
    "mesh = Meshes([verts], [faces.verts_idx], textures)\n",
    "mesh = mesh.to(device)\n",
    "R, T = look_at_view_transform(elev=25, azim=0, dist=1.4, device=device)\n",
    "image = torch.clamp(flat_renderer(mesh, R=R, T=T , device=device), 0, 1)\n",
    "image*= 255\n",
    "image_np = image[...,:3].squeeze().detach().cpu().numpy().astype(np.uint8)\n",
    "plt.imshow(image_np)\n",
    "plt.show()\n",
    "image_pil = Image.fromarray(image_np)\n",
    "image_pil.save('test.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch3d.ops import sample_points_from_meshes\n",
    "from pytorch3d.loss import chamfer_distance\n",
    "\n",
    "def scale_meshes(pred_meshes, gt_meshes, scale='gt-10'):\n",
    "    if isinstance(scale, float):\n",
    "        # Assume scale is a single scalar to use for both preds and GT\n",
    "        pred_scale = gt_scale = scale\n",
    "    elif isinstance(scale, tuple):\n",
    "        # Rescale preds and GT with different scalars\n",
    "        pred_scale, gt_scale = scale\n",
    "    elif scale.startswith(\"gt-\"):\n",
    "        # Rescale both preds and GT so that the largest edge length of each GT\n",
    "        # mesh is target\n",
    "        target = float(scale[3:])\n",
    "        bbox = gt_meshes.get_bounding_boxes()  # (N, 3, 2)\n",
    "        long_edge = (bbox[:, :, 1] - bbox[:, :, 0]).max(dim=1)[0]  # (N,)\n",
    "        scale = target / long_edge\n",
    "        if scale.numel() == 1:\n",
    "            scale = scale.expand(len(pred_meshes))\n",
    "        pred_scale, gt_scale = scale, scale\n",
    "        print(pred_scale, gt_scale)\n",
    "    else:\n",
    "        raise ValueError(\"Invalid scale: %r\" % scale)\n",
    "    pred_meshes = pred_meshes.scale_verts(pred_scale)\n",
    "    gt_meshes = gt_meshes.scale_verts(gt_scale)\n",
    "    return pred_meshes, gt_meshes\n",
    "\n",
    "obj = 'chair'\n",
    "pmo_path = f\"../../../photometric-mesh-optim/optimized_mesh/{obj}\"\n",
    "shapenet_path = f\"data/renders/test_{obj}_shapenet\"\n",
    "\n",
    "cl = []\n",
    "\n",
    "for elem in os.listdir(pmo_path):\n",
    "    car_num = elem.split('.')[0]\n",
    "    pmo_path_mesh = join(pmo_path, elem)\n",
    "    mesh_stuff = torch.load(pmo_path_mesh, map_location=device)\n",
    "    verts = mesh_stuff['vertices']\n",
    "    faces = mesh_stuff['faces']\n",
    "    center = verts.mean(0)\n",
    "    verts = verts - center\n",
    "    scale = max(verts.abs().max(0)[0])\n",
    "    verts = verts / scale\n",
    "    verts_rgb = torch.ones_like(verts)\n",
    "    textures = TexturesVertex(verts_features=[verts_rgb.to(device)])\n",
    "    mesh = Meshes(verts=[verts], faces=[faces], textures=textures)\n",
    "    mesh_pred = mesh.to(device)\n",
    "    manager = RenderManager.from_directory(int(car_num), shapenet_path)\n",
    "    mesh_info = manager.metadata[\"mesh_info\"]\n",
    "    path = f\"../data/ShapeNetCorev2/{mesh_info['synset_id']}/{mesh_info['mesh_id']}/models/model_normalized.obj\"\n",
    "    try:\n",
    "        verts, faces, aux = load_obj(path, load_textures=True, create_texture_atlas=True)\n",
    "\n",
    "        mesh_gt = Meshes(\n",
    "            verts=[verts],\n",
    "            faces=[faces.verts_idx],\n",
    "            textures=TexturesAtlas(atlas=[aux.texture_atlas]),\n",
    "        ).to(device)\n",
    "    except Exception as e:\n",
    "        mesh_gt = None\n",
    "        print('CANNOT COMPUTE CHAMFER LOSS', e)\n",
    "        continue\n",
    "    \n",
    "    mesh_pred_compute, mesh_gt_compute = scale_meshes(mesh_pred.clone(), mesh_gt.clone())\n",
    "    pcl_pred = sample_points_from_meshes(mesh_pred_compute, num_samples = 5000)\n",
    "    pcl_gt = sample_points_from_meshes(mesh_gt_compute, num_samples = 5000)\n",
    "    chamfer_loss = chamfer_distance(pcl_pred, pcl_gt, point_reduction='mean')\n",
    "    print(chamfer_loss)\n",
    "    cl.append(chamfer_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cl = [c[0] for c in cl]\n",
    "cl_tot = sum(cl) / len(cl)\n",
    "print(cl_tot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
