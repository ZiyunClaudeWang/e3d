{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Imports and dependencies\n",
    "import os\n",
    "from os.path import join\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import math\n",
    "import pandas as pd\n",
    "import imageio\n",
    "from skimage import img_as_ubyte\n",
    "from scipy.ndimage.morphology import binary_dilation\n",
    "from itertools import product\n",
    "from typing import List\n",
    "from tqdm import tqdm_notebook\n",
    "from pytorch3d.io import load_obj, save_obj\n",
    "from pytorch3d.structures import Meshes, Textures\n",
    "from pytorch3d.transforms import Rotate, Translate\n",
    "from pytorch3d.utils import ico_sphere\n",
    "from pytorch3d.ops import sample_points_from_meshes\n",
    "from pytorch3d.renderer import (\n",
    "    SfMPerspectiveCameras, OpenGLPerspectiveCameras, look_at_view_transform, look_at_rotation,\n",
    "    RasterizationSettings, MeshRenderer, MeshRasterizer, BlendParams,\n",
    "    SoftSilhouetteShader, HardPhongShader, PointLights\n",
    ")\n",
    "from pytorch3d.loss import (\n",
    "    mesh_laplacian_smoothing, \n",
    "    mesh_normal_consistency\n",
    ")\n",
    "from dataclasses import dataclass, field, asdict, astuple\n",
    "import numpy as np\n",
    "#Plotting Libs\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "from datetime import datetime\n",
    "import time\n",
    "from copy import deepcopy\n",
    "\n",
    "from plotly.subplots import make_subplots\n",
    "import plotly.graph_objects as go\n",
    "import plotly.express as px\n",
    "\n",
    "from utils.visualization import plot_pointcloud\n",
    "from utils.shapes import Sphere, SphericalSpiral\n",
    "from utils.manager import RenderManager, ImageManager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Matplotlib config nums\n",
    "mpl.rcParams['savefig.dpi'] = 90\n",
    "mpl.rcParams['figure.dpi'] = 90\n",
    "#Set the device\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "if device == \"cuda:0\": torch.cuda.set_device()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load the object without textures and materials\n",
    "verts, faces_idx, _ = load_obj(\"data/meshes/plane.obj\")\n",
    "faces = faces_idx.verts_idx\n",
    "\n",
    "# Scale normalize the target mesh to fit in a sphere of radius 1 centered at (0,0,0)\n",
    "center = verts.mean(0)\n",
    "verts = verts - center\n",
    "scale = max(verts.abs().max(0)[0])\n",
    "verts = verts / scale\n",
    "\n",
    "# Initialize each vertex to be white in color.\n",
    "verts_rgb = torch.ones_like(verts)[None]  # (1, V, 3)\n",
    "textures = Textures(verts_rgb=verts_rgb.to(device))\n",
    "\n",
    "# Create a Meshes object for the teapot. Here we have only one mesh in the batch.\n",
    "teapot_mesh = Meshes(\n",
    "    verts=[verts.to(device)],   \n",
    "    faces=[faces.to(device)], \n",
    "    textures=textures\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a Renderer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cameras = SfMPerspectiveCameras(device=device)\n",
    "\n",
    "\n",
    "# To blend the 100 faces we set a few parameters which control the opacity and the sharpness of \n",
    "# edges. Refer to blending.py for more details. \n",
    "blend_params = BlendParams(sigma=1e-4, gamma=1e-4)\n",
    "\n",
    "# Define the settings for rasterization and shading. Here we set the output image to be of size\n",
    "# 256x256. To form the blended image we use 100 faces for each pixel. We also set bin_size and max_faces_per_bin to None which ensure that \n",
    "# the faster coarse-to-fine rasterization method is used. Refer to rasterize_meshes.py for \n",
    "# explanations of these parameters. Refer to docs/notes/renderer.md for an explanation of \n",
    "# the difference between naive and coarse-to-fine rasterization. \n",
    "raster_settings = RasterizationSettings(\n",
    "    image_size=256, \n",
    "    blur_radius=np.log(1. / 1e-4 - 1.) * blend_params.sigma, \n",
    "    faces_per_pixel=100, \n",
    ")\n",
    "\n",
    "# Create a silhouette mesh renderer by composing a rasterizer and a shader. \n",
    "silhouette_renderer = MeshRenderer(\n",
    "    rasterizer=MeshRasterizer(\n",
    "        cameras=cameras, \n",
    "        raster_settings=raster_settings\n",
    "    ),\n",
    "    shader=SoftSilhouetteShader(blend_params=blend_params)\n",
    ")\n",
    "\n",
    "\n",
    "# We will also create a phong renderer. This is simpler and only needs to render one face per pixel.\n",
    "raster_settings = RasterizationSettings(\n",
    "    image_size=128, \n",
    "    blur_radius=0.0, \n",
    "    faces_per_pixel=100, \n",
    ")\n",
    "# We can add a point light in front of the object. \n",
    "lights = PointLights(device=device, location=((2.0, 2.0, -2.0),))\n",
    "phong_renderer = MeshRenderer(\n",
    "    rasterizer=MeshRasterizer(\n",
    "        cameras=cameras, \n",
    "        raster_settings=raster_settings\n",
    "    ),\n",
    "    shader=HardPhongShader(device=device, lights=lights, cameras=cameras)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Event Renderer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ON = 254\n",
    "OFF = 0\n",
    "threshold = 254.5/255\n",
    "\n",
    "def gray(img):\n",
    "    return np.dot(img[...,:3], [0.2989, 0.5870, 0.1140])\n",
    "\n",
    "def event_renderer(img1, img2, render_type):\n",
    "    \n",
    "    if render_type == \"phong\":\n",
    "        img1 = gray(img1)\n",
    "        img2 = gray(img2)\n",
    "        \n",
    "    diff_frames = img2 - img1\n",
    "    \n",
    "    threshold_diff = (diff_frames < threshold) * diff_frames\n",
    "\n",
    "    tanh_diff = np.tanh(threshold_diff)\n",
    "    #tanh_diff = np.where(tanh_diff != 0, OFF, ON)\n",
    "    \n",
    "    return tanh_diff"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a trajectory and Render"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Notes: benefit of having varying radiis for trajectory\n",
    "#TODO: Move all the camera stuff to a dataclass\n",
    "\n",
    "#Just to put these somewhere - this should be stored in param file after\n",
    "\n",
    "def cam_trajectory_rotation(num_points: int = 4):\n",
    "    \"\"\"\n",
    "    Returns: list of camera poses (R,T) from trajectory along a spherical spiral\n",
    "    \"\"\"\n",
    "    \n",
    "    shape = SphericalSpiral(\n",
    "        c = 6,\n",
    "        a = 3,\n",
    "        t_min = 1*math.pi,\n",
    "        t_max=1.05*math.pi,\n",
    "        num_points=num_points)\n",
    "    up = torch.tensor([[1., 0., 0.]])\n",
    "    R = []\n",
    "    T = []\n",
    "    for cp in shape._tuples:\n",
    "        cp = torch.tensor(cp).to(device)\n",
    "        R_new = look_at_rotation(cp[None, :], device=device)\n",
    "        T_new = -torch.bmm(R_new.transpose(1,2), cp[None, :, None])[:, :, 0]\n",
    "        if not len(R) and not len(T):\n",
    "            R = [R_new]\n",
    "            T = [T_new]\n",
    "        else:\n",
    "            R.append(R_new)\n",
    "            T.append(T_new)\n",
    "    return (torch.stack(R)[:,0,:], torch.stack(T)[:,0,:])\n",
    "\n",
    "def cam_trajectory_transform(\n",
    "    dist: float = 1.5,\n",
    "    elev_range: list= 55.0,\n",
    "    azim_range: list= [0, 180],\n",
    "    batch_size: int = 64):\n",
    "    \"\"\"\n",
    "    Input\n",
    "    Returns cameras from ranges of elevations and azimuths\n",
    "    \n",
    "    Not going to use this for now\n",
    "    \"\"\"\n",
    "    \n",
    "    elev = torch.tensor([elev_range] * batch_size)\n",
    "    idx = 0\n",
    "    pepper = [-1., -.75, -.5, -.25, 0, .25, .5, .75, 1.]\n",
    "    for e in range(len(elev)):\n",
    "        curr_idx = idx % len(pepper)\n",
    "        elev[e] = elev[e] + pepper[curr_idx]\n",
    "        idx += 1\n",
    "    print(elev)\n",
    "    azim = torch.linspace(azim_range[0], azim_range[1], batch_size)\n",
    "    R, T = look_at_view_transform(dist=dist, elev=elev, azim=azim, device=device)\n",
    "    return (R, T)\n",
    "    \n",
    "def render_trajectory(cam_poses, write_gif=True):\n",
    "    \n",
    "    renders = {\n",
    "        \"phong\": None,\n",
    "        \"silhouette\": None,\n",
    "        \"events\": None\n",
    "    }\n",
    "    render_manager = RenderManager(\n",
    "        types=list(renders.keys()),\n",
    "        mesh_name = \"teapot\"\n",
    "    )\n",
    "    render_manager.init()\n",
    "    # Render the teapot providing the values of R and T.\n",
    "    R, T = cam_poses\n",
    "    for num in range(1, len(R) + 1):\n",
    "        if \"phong\" in renders.keys():\n",
    "            image_ref = phong_renderer(meshes_world=teapot_mesh, R=R[num-1:num:], T=T[num-1:num:])\n",
    "            image_ref = image_ref.cpu().numpy()\n",
    "            plt.subplot(131)\n",
    "            plt.imshow(image_ref.squeeze())\n",
    "        if \"silhouette\" in renders.keys():\n",
    "            silhouette = silhouette_renderer(meshes_world=teapot_mesh, R=R[num-1:num:], T=T[num-1:num:])\n",
    "            silhouette = silhouette.cpu().numpy()\n",
    "            plt.subplot(132)\n",
    "            plt.imshow(silhouette.squeeze()[...,3])  # only plot the alpha channel of the RGBA image \n",
    "        render_manager.add_images(\n",
    "            num,\n",
    "            {\"silhouette\": silhouette.squeeze()[...,3], \"phong\": image_ref.squeeze()},\n",
    "           R[num-1:num:], T[num-1:num:])\n",
    "        if \"events\" in renders.keys():\n",
    "            if num == 1:\n",
    "                #prev_img_sil = silhouette\n",
    "                prev_img_ref = image_ref\n",
    "                plt.show()\n",
    "                continue\n",
    "            #sil_event_frame = event_renderer(prev_img_sil.squeeze(), silhouette.squeeze(), \"silhouette\")\n",
    "            #We're generating with phong because it's closer to \"real\"\n",
    "            ref_event_frame = event_renderer(prev_img_ref.squeeze(), image_ref.squeeze(), \"phong\")            \n",
    "            plt.subplot(133)\n",
    "            plt.imshow(ref_event_frame)\n",
    "            render_manager.add_event_frame(num, ref_event_frame)\n",
    "            #prev_img_sil = silhouette\n",
    "            prev_img_ref = image_ref\n",
    "        plt.show()\n",
    "        \n",
    "    render_manager.close()\n",
    "    return render_manager\n",
    "\n",
    "    \n",
    "cam_poses = cam_trajectory_transform()\n",
    "render = render_trajectory(cam_poses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Diff Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def neg_iou_loss(predict, target):\n",
    "    dims = tuple(range(predict.ndimension())[1:])\n",
    "    intersect = (predict * target).sum(dims)\n",
    "    union = (predict + target - predict * target).sum(dims) + 1e-6\n",
    "    return 1. - (intersect / union).sum() / intersect.nelement()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MeshDeformationModel(nn.Module):\n",
    "    \n",
    "    def __init__(self, device, template_mesh = None):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.device = device\n",
    "        \n",
    "        #Create a source mesh\n",
    "        if not template_mesh:\n",
    "            template_mesh = ico_sphere(2, device)\n",
    "        \n",
    "        verts, faces = template_mesh.get_mesh_verts_faces(0)\n",
    "        #Initialize each vert to have no tetxture\n",
    "        verts_rgb = torch.ones_like(verts)[None]\n",
    "        textures = Textures(verts_rgb=verts_rgb.to(self.device))\n",
    "        self.template_mesh = Meshes(\n",
    "            verts=[verts.to(self.device) * 1.5],\n",
    "            faces=[faces.to(self.device)],\n",
    "            textures = textures\n",
    "        )\n",
    "        \n",
    "        self.register_buffer('vertices', self.template_mesh.verts_padded())\n",
    "        self.register_buffer('faces', self.template_mesh.faces_padded())\n",
    "        self.register_buffer('textures', textures.verts_rgb_padded())\n",
    "        \n",
    "        deform_verts = torch.zeros_like(self.template_mesh.verts_packed(), device=device, requires_grad=True)\n",
    "        #deform_verts = torch.full(self.template_mesh.verts_packed().shape, 0.0, device=device, requires_grad=True)\n",
    "        #Create an optimizable parameter for the mesh\n",
    "        self.register_parameter('deform_verts', nn.Parameter(deform_verts).to(self.device))\n",
    "        \n",
    "        laplacian_loss = mesh_laplacian_smoothing(template_mesh, method=\"uniform\")\n",
    "        flatten_loss = mesh_normal_consistency(template_mesh)\n",
    "        \n",
    "    def forward(self, batch_size):\n",
    "        #Offset the mesh\n",
    "        deformed_mesh_verts = self.template_mesh.offset_verts(self.deform_verts)\n",
    "        texture = Textures(verts_rgb = self.textures)\n",
    "        deformed_mesh = Meshes(verts=deformed_mesh_verts.verts_padded(), faces=deformed_mesh_verts.faces_padded(), textures=texture)\n",
    "        deformed_meshes = deformed_mesh.extend(batch_size)\n",
    "    \n",
    "        laplacian_loss = mesh_laplacian_smoothing(deformed_mesh, method=\"uniform\")\n",
    "        flatten_loss = mesh_normal_consistency(deformed_mesh)\n",
    "        \n",
    "        return deformed_meshes, laplacian_loss, flatten_loss\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EVENT-BASED Optimization \n",
    "\n",
    "### Initialize the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weight_ssd = 1\n",
    "weight_laplacian = .01\n",
    "weight_flatten = .0001\n",
    "\n",
    "batch_size = 128\n",
    "\n",
    "# We will save images periodically and compose them into a GIF.\n",
    "filename_output = \"./projection_loss.gif\"\n",
    "writer = imageio.get_writer(filename_output, mode='I', duration=0.3)\n",
    "\n",
    "# Initialize a model using the renderer, template mesh and reference image\n",
    "model = MeshDeformationModel(device).to(device)\n",
    "\n",
    "# Create an optimizer. Here we are using Adam and we pass in the parameters of the model\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=.001, betas=(0.5, 0.99)) #Hyperparameter tuning\n",
    "\n",
    "render = RenderManager.from_directory(dir_num=60)\n",
    "R, T = render._trajectory\n",
    "#images_gt = render._images(type_key = \"silhouette\").to(device)\n",
    "events = torch.true_divide(render._events().to(device), 255)\n",
    "print(events.shape)\n",
    "cameras = SfMPerspectiveCameras(device=device, R=R, T=T)\n",
    "print(R.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run the optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loop = tqdm_notebook(range(2000))\n",
    "laplacian_losses = []\n",
    "flatten_losses = []\n",
    "ssd_losses = []\n",
    "\n",
    "for i in loop:\n",
    "    \n",
    "    mesh, laplacian_loss, flatten_loss = model(batch_size)\n",
    "\n",
    "    #The events were also generated with phong renderer\n",
    "    images_pred = phong_renderer(mesh.clone(), device=device, cameras=cameras)\n",
    "    events_pred = []\n",
    "    prev_img = images_pred[0]\n",
    "    for img in images_pred[1:-1]:\n",
    "        ev = event_renderer(prev_img.detach().cpu().squeeze(), img.detach().cpu().squeeze(), \"phong\")\n",
    "        events_pred.append(torch.from_numpy(ev))\n",
    "    events_pred = torch.stack(events_pred).to(device)\n",
    "    \n",
    "    #silhouette_loss = neg_iou_loss(images_gt, images_pred)\n",
    "    ssd_loss = torch.sum((events[0:batch_size-2] - events_pred) ** 2)\n",
    "    \n",
    "    loss = ssd_loss * weight_ssd + laplacian_loss * weight_laplacian + flatten_loss * weight_flatten\n",
    "    \n",
    "    loop.set_description('Optimizing (loss %.4f)' % loss.data)\n",
    "    \n",
    "    ssd_losses.append(ssd_loss * weight_ssd)\n",
    "    laplacian_losses.append(laplacian_loss * weight_laplacian)\n",
    "    flatten_losses.append(flatten_loss * weight_flatten)\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    if i % 100 == 0:\n",
    "        #Write images\n",
    "        image = images_pred.detach().cpu().numpy()[0]\n",
    "        writer.append_data((255*image).astype(np.uint8))\n",
    "        imageio.imsave(f\"data/dump/mesh_{i}.png\", (255*image).astype(np.uint8))\n",
    "        \n",
    "        f, (ax1, ax2) = plt.subplots(1, 2)\n",
    "        \n",
    "        image = img_as_ubyte(image)\n",
    "        ax1.imshow(image)\n",
    "        ax1.set_title(\"Deformed Mesh\")\n",
    "        \n",
    "        ax2.plot(silhouette_losses, label=\"Silhouette Loss\")\n",
    "        ax2.plot(laplacian_losses, label=\"Laplacian Loss\")\n",
    "        ax2.plot(flatten_losses, label=\"Flatten Loss\")\n",
    "        ax2.legend(fontsize=\"16\")\n",
    "        ax2.set_xlabel(\"Iteration\", fontsize=\"16\")\n",
    "        ax2.set_ylabel(\"Loss\", fontsize=\"16\")\n",
    "        ax2.set_title(\"Loss vs iterations\", fontsize=\"16\")\n",
    "        \n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch3d",
   "language": "python",
   "name": "pytorch3d"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
