{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Imports and dependencies\n",
    "import os\n",
    "from os.path import join\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import math\n",
    "import pandas as pd\n",
    "import imageio\n",
    "from skimage import img_as_ubyte\n",
    "from scipy.ndimage.morphology import binary_dilation\n",
    "from itertools import product\n",
    "from typing import List\n",
    "from tqdm import tqdm_notebook\n",
    "from pytorch3d.io import load_obj, save_obj\n",
    "from pytorch3d.structures import Meshes, Textures\n",
    "from pytorch3d.transforms import Rotate, Translate\n",
    "from pytorch3d.utils import ico_sphere\n",
    "from pytorch3d.ops import sample_points_from_meshes\n",
    "from pytorch3d.renderer import (\n",
    "    SfMPerspectiveCameras, OpenGLPerspectiveCameras, look_at_view_transform, look_at_rotation,\n",
    "    RasterizationSettings, MeshRenderer, MeshRasterizer, BlendParams,\n",
    "    SoftSilhouetteShader, HardPhongShader, PointLights\n",
    ")\n",
    "from dataclasses import dataclass, field, asdict, astuple\n",
    "import numpy as np\n",
    "#Plotting Libs\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "from datetime import datetime\n",
    "import time\n",
    "from copy import deepcopy\n",
    "\n",
    "from plotly.subplots import make_subplots\n",
    "import plotly.graph_objects as go\n",
    "import plotly.express as px\n",
    "\n",
    "from utils.visualization import plot_pointcloud\n",
    "from utils.shapes import Sphere, SphericalSpiral\n",
    "from utils.manager import RenderManager, ImageManager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Matplotlib config nums\n",
    "mpl.rcParams['savefig.dpi'] = 80\n",
    "mpl.rcParams['figure.dpi'] = 80\n",
    "#Set the device\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "if device == \"cuda:0\": torch.cuda.set_device()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Download a couple meshes to work with\n",
    "#!wget -P data/meshes https://dl.fbaipublicfiles.com/pytorch3d/data/dolphin/dolphin.obj\n",
    "#!wget -P data/meshes https://dl.fbaipublicfiles.com/pytorch3d/data/teapot/teapot.obj\n",
    "!wget -P data/meshes https://dl.fbaipublicfiles.com/pytorch3d/data/plane/plane.obj"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load the object without textures and materials\n",
    "verts, faces_idx, _ = load_obj(\"data/meshes/teapot.obj\")\n",
    "faces = faces_idx.verts_idx\n",
    "\n",
    "# Initialize each vertex to be white in color.\n",
    "verts_rgb = torch.ones_like(verts)[None]  # (1, V, 3)\n",
    "textures = Textures(verts_rgb=verts_rgb.to(device))\n",
    "\n",
    "# Create a Meshes object for the teapot. Here we have only one mesh in the batch.\n",
    "teapot_mesh = Meshes(\n",
    "    verts=[verts.to(device)],   \n",
    "    faces=[faces.to(device)], \n",
    "    textures=textures\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a renderer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cameras = SfMPerspectiveCameras(device=device)\n",
    "\n",
    "\n",
    "# To blend the 100 faces we set a few parameters which control the opacity and the sharpness of \n",
    "# edges. Refer to blending.py for more details. \n",
    "blend_params = BlendParams(sigma=1e-4, gamma=1e-4)\n",
    "\n",
    "# Define the settings for rasterization and shading. Here we set the output image to be of size\n",
    "# 256x256. To form the blended image we use 100 faces for each pixel. We also set bin_size and max_faces_per_bin to None which ensure that \n",
    "# the faster coarse-to-fine rasterization method is used. Refer to rasterize_meshes.py for \n",
    "# explanations of these parameters. Refer to docs/notes/renderer.md for an explanation of \n",
    "# the difference between naive and coarse-to-fine rasterization. \n",
    "raster_settings = RasterizationSettings(\n",
    "    image_size=256, \n",
    "    blur_radius=np.log(1. / 1e-4 - 1.) * blend_params.sigma, \n",
    "    faces_per_pixel=100, \n",
    ")\n",
    "\n",
    "# Create a silhouette mesh renderer by composing a rasterizer and a shader. \n",
    "silhouette_renderer = MeshRenderer(\n",
    "    rasterizer=MeshRasterizer(\n",
    "        cameras=cameras, \n",
    "        raster_settings=raster_settings\n",
    "    ),\n",
    "    shader=SoftSilhouetteShader(blend_params=blend_params)\n",
    ")\n",
    "\n",
    "\n",
    "# We will also create a phong renderer. This is simpler and only needs to render one face per pixel.\n",
    "raster_settings = RasterizationSettings(\n",
    "    image_size=256, \n",
    "    blur_radius=0.0, \n",
    "    faces_per_pixel=100, \n",
    ")\n",
    "# We can add a point light in front of the object. \n",
    "lights = PointLights(device=device, location=((2.0, 2.0, -2.0),))\n",
    "phong_renderer = MeshRenderer(\n",
    "    rasterizer=MeshRasterizer(\n",
    "        cameras=cameras, \n",
    "        raster_settings=raster_settings\n",
    "    ),\n",
    "    shader=HardPhongShader(device=device, lights=lights, cameras=cameras)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ref - look_at_view_transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select the viewpoint using spherical angles  \n",
    "distance = 3   # distance from camera to the object\n",
    "elevation = 50.0   # angle of elevation in degrees\n",
    "azimuth = 0.0  # No rotation so the camera is positioned on the +Z axis. \n",
    "\n",
    "# Get the position of the camera based on the spherical angles\n",
    "R, T = look_at_view_transform(distance, elevation, azimuth, device=device)\n",
    "\n",
    "# Render the teapot providing the values of R and T. \n",
    "silhouete = silhouette_renderer(meshes_world=teapot_mesh, R=R, T=T)\n",
    "image_ref = phong_renderer(meshes_world=teapot_mesh, R=R, T=T)\n",
    "\n",
    "silhouete = silhouete.cpu().numpy()\n",
    "image_ref = image_ref.cpu().numpy()\n",
    "\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.imshow(silhouete.squeeze()[..., 3])  # only plot the alpha channel of the RGBA image\n",
    "plt.grid(False)\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.imshow(image_ref.squeeze())\n",
    "plt.grid(False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ref - look_at_rotation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Position of the camera in world coordinates\n",
    "x = 2.1\n",
    "y = 3.4\n",
    "z = 0.5\n",
    "cam_posn = torch.from_numpy(np.array([x, y, z], dtype=np.float32))\n",
    "#at - position of the object in world coordinates \n",
    "#up - vector of up direction in world coordinates\n",
    "R = look_at_rotation(cam_posn[None, :], device=device)\n",
    "T = -torch.bmm(R.transpose(1,2), cam_posn[None, :, None])[:, :, 0]\n",
    "\n",
    "# Render the teapot providing the values of R and T. \n",
    "silhouete = silhouette_renderer(meshes_world=teapot_mesh, R=R, T=T, camera)\n",
    "image_ref = phong_renderer(meshes_world=teapot_mesh, R=R, T=T)\n",
    "\n",
    "silhouete = silhouete.cpu().numpy()\n",
    "image_ref = image_ref.cpu().numpy()\n",
    "\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.imshow(silhouete.squeeze()[..., 3])  # only plot the alpha channel of the RGBA image\n",
    "plt.grid(False)\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.imshow(image_ref.squeeze())\n",
    "plt.grid(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Sphere(\n",
    "    radius=2,\n",
    "    num_points_theta = 1,\n",
    "    num_points_phi =20,\n",
    "    theta_min = math.pi/2,\n",
    "    theta_max = math.pi/2, \n",
    "    phi_max = math.pi).plot()\n",
    "\n",
    "SphericalSpiral(\n",
    "    c = 6.5, \n",
    "    a = 1.5,\n",
    "    t_min = math.pi,\n",
    "    t_max=2*math.pi,\n",
    "    num_points=1000).plot()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Camera Pose Trajectory and render"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Notes: benefit of having varying radiis for trajectory\n",
    "#TODO: Move all the camera stuff to a dataclass\n",
    "\n",
    "#Just to put these somewhere - this should be stored in param file after\n",
    "\n",
    "def cam_trajectory_rotation(num_points: int = 3):\n",
    "    \"\"\"\n",
    "    Returns: list of camera poses (R,T) from trajectory along a spherical spiral\n",
    "    \"\"\"\n",
    "    \n",
    "    #sphere = Sphere(\n",
    "    #    radius=2,\n",
    "    #    num_points_theta = 1,\n",
    "    #    num_points_phi = 30,\n",
    "    #    theta_min = math.pi/2,\n",
    "    #    theta_max = math.pi/2, \n",
    "    #    phi_max = math.pi)\n",
    "    shape = SphericalSpiral(\n",
    "        c = 6.5,\n",
    "        a = 1.5,\n",
    "        t_min = .2 * math.pi,\n",
    "        t_max= .25 * math.pi,\n",
    "        num_points=num_points)\n",
    "    up = torch.tensor([[0., 1., 1.]])\n",
    "    R = []\n",
    "    T = []\n",
    "    for cp in shape._tuples:\n",
    "        cp = torch.tensor(cp)\n",
    "        R_new = look_at_rotation(cp[None, :],up=up, device=device)\n",
    "        T_new = -torch.bmm(R_new.transpose(1,2), cp[None, :, None])[:, :, 0]\n",
    "        if not len(R) and not len(T):\n",
    "            R = [R_new]\n",
    "            T = [T_new]\n",
    "        else:\n",
    "            R.append(R_new)\n",
    "            T.append(T_new)\n",
    "    return (R, T)\n",
    "\n",
    "def cam_trajectory_transform(\n",
    "    dist: float = 2.7,\n",
    "    elev_range: list= [10, 50],\n",
    "    azim_range: list= [-180, 180]):\n",
    "    \"\"\"\n",
    "    Input\n",
    "    Returns cameras from ranges of elevations and azimuths\n",
    "    \n",
    "    Not going to use this for now\n",
    "    \"\"\"\n",
    "    \n",
    "    elev = torch.linspace(elev_range[0], elev_range[1], batch_size)\n",
    "    azim = torch.linspace(elev_range[0], elev_range[1], batch_size)\n",
    "    R = []\n",
    "    T = []\n",
    "    for e, a in elev, azim:\n",
    "        R_new, T_new = look_at_view_transform(dist=dist, elev=e, azim=a)\n",
    "        R = torch.stack((R, R_new)) if R else R_new\n",
    "        T = torch.stack((T, T_new)) if T else T_new\n",
    "    return (R, T)\n",
    "    \n",
    "def render_trajectory(cam_poses, write_gif=True):\n",
    "    \n",
    "    \"\"\"\n",
    "    output_folder = \"./data/renders/\"\n",
    "    \n",
    "    silhouette_output = join(output_folder, \"camera_simulation_silhouette.gif\")\n",
    "    silhouette_writer = imageio.get_writer(silhouette_output, mode=\"I\", duration=.2)\n",
    "    \n",
    "    image_output = join(output_folder, \"camera_simulation_image.gif\")\n",
    "    image_writer = imageio.get_writer(image_output, mode=\"I\", duration=.2)\n",
    "    \"\"\"\n",
    "    render_manager = RenderManager(\n",
    "        types=[\"silhouette\"],\n",
    "        mesh_name = \"teapot\"\n",
    "    )\n",
    "    # Render the teapot providing the values of R and T.\n",
    "    R, T = cam_poses\n",
    "    for num in range(len(R)):\n",
    "        #gl_cam = OpenGLPerspectiveCameras(device=device, R=R, T=T)\n",
    "        #sf_cam = SfMPerspectiveCameras(device=device, R=R, T=T)\n",
    "        silhouette = silhouette_renderer(meshes_world=teapot_mesh, R=R[num], T=T[num])\n",
    "        image_ref = phong_renderer(meshes_world=teapot_mesh, R=R[num], T=T[num])\n",
    "        \n",
    "        silhouette = silhouette.cpu().numpy()\n",
    "        image_ref = image_ref.cpu().numpy()\n",
    "        \n",
    "        plt.figure(figsize=(10, 10))\n",
    "        plt.subplot(1, 2, 1)\n",
    "        plt.imshow(silhouette.squeeze()[...,3])  # only plot the alpha channel of the RGBA image\n",
    "        \n",
    "        plt.grid(False)\n",
    "        plt.subplot(1, 2, 2)\n",
    "        plt.imshow(image_ref.squeeze())\n",
    "        plt.grid(False)\n",
    "        \n",
    "        plt.show()\n",
    "        render_manager.add_images(\n",
    "            num,\n",
    "            {\"silhouette\": silhouette.squeeze()}, \n",
    "            R[num], T[num])\n",
    "    \n",
    "        \n",
    "    render_manager.close()\n",
    "    return render_manager\n",
    "    \n",
    "cam_poses = cam_trajectory_rotation()\n",
    "render = render_trajectory(cam_poses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate events from consecutive frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ON = 255\n",
    "OFF = 0\n",
    "threshold = 250\n",
    "\n",
    "def rgb2gray(rgb):\n",
    "    return np.dot(rgb[...,:3], [0.2989, 0.5870, 0.1140])\n",
    "\n",
    "def adaptive_thresholding(diff_frames):\n",
    "    \"\"\"\n",
    "    Experiment: \n",
    "        Determine threshold from mean background illumination\n",
    "    \"\"\"\n",
    "    raise NotImplementedError\n",
    "\n",
    "def dilate_boundaries(diff_frame):\n",
    "    \"\"\"\n",
    "    Experiment:\n",
    "        Dilate the boundaries to introduce more noise on the outside\n",
    "    \"\"\"\n",
    "    raise NotImplementedError\n",
    "\n",
    "def fiter_consts(array_frame):\n",
    "    const_filter = [float(\"nan\"), float(\"inf\")] #Just so we can extend this maybe?\n",
    "    for const in const_filter:\n",
    "        rows, cols = np.where(array_frame == const)\n",
    "\n",
    "filename_output = \"./gen_events.gif\"\n",
    "writer = imageio.get_writer(filename_output, mode='I', duration=.2, loop=0)    \n",
    "\n",
    "for type_key in render.images.keys():\n",
    "    prev_img = None\n",
    "    for num, img in enumerate(render.images[type_key]):\n",
    "        img = deepcopy(img)\n",
    "        img_manager = ImageManager.from_dict(img)        \n",
    "        img_data = np.array(img_manager._load)\n",
    "        #if type_key == \"shaded\":\n",
    "        #    img_data = rgb2gray(img_data)\n",
    "        #else: continue\n",
    "        if prev_img is not None:\n",
    "            \n",
    "            #if type_key == \"shaded\": print(np.where(img_data!=prev_img))\n",
    "            \n",
    "            diff_frames = np.subtract(img_data, prev_img)\n",
    "            #if type_key == \"shaded\": \n",
    "                #Plt imshow should only take in a uint8\n",
    "            #    diff_frames = np.uint8(diff_frames)\n",
    "            \n",
    "            #diff_frames[:,:,-1].shape) - only take first two dims of the array\n",
    "            #print(np.where(diff_frames==float('inf')))\n",
    "                \n",
    "            threshold_diff = (diff_frames < threshold) * diff_frames\n",
    "            #threshold_diff = np.where(diff_frames > threshold, 1, 0) #This sets the entire array to 1 or 0\n",
    "            \n",
    "            tanh_diff = np.tanh(threshold_diff).astype(np.uint8)\n",
    "            tanh_diff = np.where(tanh_diff != 0, ON, OFF) #Set the pixels to on or off based off of their value\n",
    "            \n",
    "            #Matplot plotting\n",
    "            plt.figure(figsize=(10, 10))\n",
    "            plt.subplot(1, 2, 1)\n",
    "            plt.imshow(prev_img) \n",
    "            plt.grid(False)\n",
    "            plt.subplot(1, 2, 2)\n",
    "            plt.imshow(tanh_diff)\n",
    "            plt.grid(False)\n",
    "            \n",
    "            image = img_as_ubyte(tanh_diff)\n",
    "            writer.append_data(image)\n",
    "            \"\"\"\n",
    "            #Plotly plotting - this doesn't work\n",
    "            fig = make_subplots(rows=1, cols=2)\n",
    "            fig.add_trace(\n",
    "                px.imshow(diff_frames),\n",
    "                row = 1, col = 1\n",
    "            )\n",
    "            fig.add_trace(\n",
    "                px.imshow(tanh_diff),\n",
    "                row=1, col=2\n",
    "            )\n",
    "            fig.show()\n",
    "            \"\"\"\n",
    "        prev_img = img_data\n",
    "writer.close()\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageBasedDifferentiableModel(nn.Module):\n",
    "    \n",
    "    def __init__(self, render, renderer, device):\n",
    "        super().__init__()\n",
    "\n",
    "        #Store stuff\n",
    "        self.device = device\n",
    "        self.renderer = renderer\n",
    "        \n",
    "        #Load images from the render\n",
    "        #trajectory = [R, T] where R & T are (N,3)\n",
    "        trajectory = render._trajectory\n",
    "        self.R, self.T = trajectory\n",
    "        #images = np.array([img1, img2]) where img is (256, 256)\n",
    "        self.register_buffer('images_gt', render._images())\n",
    "        #self.images_gt = render._images()\n",
    "        #print(self.images_gt)\n",
    "        self.cameras = SfMPerspectiveCameras(device=self.device, R=self.R, T=self.T)\n",
    "        \n",
    "        #Register to the buffer\n",
    "        #self.register_buffer('trajectory', trafjectory)\n",
    "        #self.register_buffer('images', images)\n",
    "        \n",
    "        #Create a source mesh\n",
    "        template_sphere = ico_sphere(3, device)\n",
    "        verts, faces = template_sphere.get_mesh_verts_faces(0)\n",
    "        #Initialize each vert to have no tetxture\n",
    "        verts_rgb = torch.ones_like(verts)[None]\n",
    "        textures = Textures(verts_rgb=verts_rgb.to(self.device))\n",
    "        self.template_mesh = Meshes(\n",
    "            verts=[verts.to(self.device)],\n",
    "            faces=[faces.to(self.device)],\n",
    "            textures = textures\n",
    "        )\n",
    "        \n",
    "        r = torch.index_select(self.R, 0, torch.tensor([0]))\n",
    "        t = torch.index_select(self.T, 0, torch.tensor([0]))\n",
    "        \n",
    "        render_mesh = self.renderer(meshes_world = self.template_mesh, device = self.device, R=r, T =t)\n",
    "        plt.imshow(render_mesh.cpu().numpy().squeeze())\n",
    "        \n",
    "        self.register_buffer('vertices', self.template_mesh.verts_padded())\n",
    "        self.register_buffer('faces', self.template_mesh.faces_padded())\n",
    "        self.register_buffer('textures', textures.verts_rgb_padded())\n",
    "        \n",
    "        deform_verts = torch.zeros_like(self.template_mesh.verts_packed(), device=device, requires_grad=True)\n",
    "        #Create an optimizable parameter for the mesh\n",
    "        self.register_parameter('deform_verts', nn.Parameter(deform_verts).to(self.device))\n",
    "        \n",
    "    def forward(self):\n",
    "        #Offset the mesh\n",
    "        deformed_mesh_verts = self.template_mesh.offset_verts(self.deform_verts)\n",
    "        texture = Textures(verts_rgb = self.textures)\n",
    "        deformed_mesh = Meshes(verts=deformed_mesh_verts.verts_padded(), faces=deformed_mesh_verts.faces_padded(), textures=texture)\n",
    "        \n",
    "        '''\n",
    "        r = torch.index_select(self.R, 0, torch.tensor([0]))\n",
    "        print(r, r.shape)\n",
    "        t = torch.index_select(self.T, 0, torch.tensor([0]))\n",
    "        print(t, t.shape)\n",
    "        render_mesh = self.renderer(meshes_world = deformed_mesh, device = self.device, R=r, T =t)\n",
    "        plt.imshow(render_mesh[0,...,:3].detach().cpu().numpy())\n",
    "        '''\n",
    "        ##BATCH Project 3D mesh to 2D ##\n",
    "        batch_size = len(self.images_gt)\n",
    "        mesh_buffer = deformed_mesh.extend(batch_size)\n",
    "        #print(mesh_buffer)\n",
    "        #cameras = OpenGLOrthographicCameras(device=self.device, R=self.R, T=self.T)\n",
    "        projections = self.renderer(mesh_buffer.clone(), device=self.device, cameras=self.cameras)\n",
    "        \n",
    "        #Calculate loss of each image\n",
    "        #Other things to try: I\n",
    "        '''\n",
    "        loss_list = []\n",
    "        for idx in range(batch_size):\n",
    "            projection = projections[idx][..., 3].detach()\n",
    "            image_gt = self.images_gt[idx]\n",
    "            img_diff = torch.sum((projection - image_gt) ** 2)\n",
    "            loss_list.append(img_diff)\n",
    "        loss = torch.stack(loss_list, dim=0, requires_grad=True)\n",
    "        '''\n",
    "        loss = torch.sum((projections[...,3] - self.images_gt) ** 2)\n",
    "        return loss, projections[0]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will save images periodically and compose them into a GIF.\n",
    "filename_output = \"./projection_loss.gif\"\n",
    "writer = imageio.get_writer(filename_output, mode='I', duration=0.3)\n",
    "\n",
    "# Initialize a model using the renderer, mesh and reference image\n",
    "model = ImageBasedDifferentiableModel(render, phong_renderer, device).to(device)\n",
    "\n",
    "# Create an optimizer. Here we are using Adam and we pass in the parameters of the model\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.05) #Hyperparameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loop = tqdm_notebook(range(200))\n",
    "for i in loop:\n",
    "    optimizer.zero_grad()\n",
    "    loss, proj = model()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    print(loss.grad)\n",
    "    \n",
    "    loop.set_description('Optimizing (loss %.4f)' % loss.data)\n",
    "    \n",
    "    if loss.item() < 200:\n",
    "        break\n",
    "    \n",
    "    # Save outputs to create a GIF. \n",
    "    if i % 10 == 0:\n",
    "        print(\"10th iter\")\n",
    "        R = model.R\n",
    "        T = model.T   # (1, 3)\n",
    "        #image = proj.detach().squeeze()\n",
    "        #image = img_as_ubyte(image)\n",
    "        #writer.append_data(image)\n",
    "        \n",
    "        #plt.figure()\n",
    "        #plt.imshow(image)\n",
    "        #plt.title(\"iter: %d, loss: %0.2f\" % (i, loss.data))\n",
    "        #plt.grid(\"off\")\n",
    "        #plt.axis(\"off\")\n",
    "        #plt.show()\n",
    "    \n",
    "#writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
