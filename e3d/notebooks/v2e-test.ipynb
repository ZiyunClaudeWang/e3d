{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from os.path import join, abspath, dirname\n",
    "import sys\n",
    "sys.path.insert(0, abspath(join(\"..\", dirname(os.getcwd()))))\n",
    "from os.path import join\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import math\n",
    "import pandas as pd\n",
    "import imageio\n",
    "from skimage import img_as_ubyte\n",
    "from scipy.ndimage.morphology import binary_dilation\n",
    "from itertools import product\n",
    "from typing import List\n",
    "from tqdm import tqdm_notebook\n",
    "from pytorch3d.io import load_obj, save_obj\n",
    "from pytorch3d.structures import Meshes\n",
    "from pytorch3d.transforms import Rotate, Translate\n",
    "from pytorch3d.utils import ico_sphere\n",
    "from pytorch3d.ops import sample_points_from_meshes\n",
    "from pytorch3d.renderer import (\n",
    "    SfMPerspectiveCameras, OpenGLPerspectiveCameras, look_at_view_transform, look_at_rotation,\n",
    "    RasterizationSettings, MeshRenderer, MeshRasterizer, BlendParams,\n",
    "    SoftSilhouetteShader, HardPhongShader, PointLights, TexturesVertex\n",
    ")\n",
    "from pytorch3d.loss import (\n",
    "    mesh_laplacian_smoothing, \n",
    "    mesh_normal_consistency\n",
    ")\n",
    "from dataclasses import dataclass, field, asdict, astuple\n",
    "import numpy as np\n",
    "import cv2\n",
    "#Plotting Libs\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "%matplotlib inline\n",
    "\n",
    "from datetime import datetime\n",
    "import time\n",
    "from copy import deepcopy\n",
    "\n",
    "from plotly.subplots import make_subplots\n",
    "import plotly.graph_objects as go\n",
    "import plotly.express as px\n",
    "\n",
    "from utils.shapes import Sphere, SphericalSpiral\n",
    "from utils.manager import RenderManager, ImageManager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from v2e.emulator import EventEmulator\n",
    "from v2e.renderer import EventRenderer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Matplotlib config nums\n",
    "mpl.rcParams['savefig.dpi'] = 90\n",
    "mpl.rcParams['figure.dpi'] = 90\n",
    "#Set the device\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "if device == \"cuda:0\": torch.cuda.set_device()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Single Obj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load the object without textures and materials\n",
    "mesh_name = \"helicopter\"\n",
    "verts, faces_idx, _ = load_obj(f\"../data/meshes/{mesh_name}.obj\")\n",
    "faces = faces_idx.verts_idx\n",
    "\n",
    "# Initialize each vertex to be white in color.\n",
    "verts_rgb = torch.ones_like(verts)[None]  # (1, V, 3)\n",
    "textures = TexturesVertex(verts_rgb.to(device))\n",
    "\n",
    "# Create a Meshes object for the teapot. Here we have only one mesh in the batch.\n",
    "teapot_mesh = Meshes(\n",
    "    verts=[verts.to(device)],   \n",
    "    faces=[faces.to(device)], \n",
    "    textures=textures\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multi Obj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### SOME BIG FAT TODO HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a renderer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cameras = SfMPerspectiveCameras(device=device)\n",
    "\n",
    "\n",
    "# To blend the 100 faces we set a few parameters which control the opacity and the sharpness of \n",
    "# edges. Refer to blending.py for more details. \n",
    "blend_params = BlendParams(sigma=1e-4, gamma=1e-4)\n",
    "\n",
    "# Define the settings for rasterization and shading. Here we set the output image to be of size\n",
    "# 256x256. To form the blended image we use 100 faces for each pixel. We also set bin_size and max_faces_per_bin to None which ensure that \n",
    "# the faster coarse-to-fine rasterization method is used. Refer to rasterize_meshes.py for \n",
    "# explanations of these parameters. Refer to docs/notes/renderer.md for an explanation of \n",
    "# the difference between naive and coarse-to-fine rasterization. \n",
    "raster_settings = RasterizationSettings(\n",
    "    image_size=512, \n",
    "    blur_radius=np.log(1. / 1e-4 - 1.) * blend_params.sigma, \n",
    "    faces_per_pixel=100, \n",
    ")\n",
    "\n",
    "# Create a silhouette mesh renderer by composing a rasterizer and a shader. \n",
    "silhouette_renderer = MeshRenderer(\n",
    "    rasterizer=MeshRasterizer(\n",
    "        cameras=cameras, \n",
    "        raster_settings=raster_settings\n",
    "    ),\n",
    "    shader=SoftSilhouetteShader(blend_params=blend_params)\n",
    ")\n",
    "\n",
    "\n",
    "# We will also create a phong renderer. This is simpler and only needs to render one face per pixel.\n",
    "raster_settings = RasterizationSettings(\n",
    "    image_size=512, \n",
    "    blur_radius=0.0, \n",
    "    faces_per_pixel=100, \n",
    ")\n",
    "# We can add a point light in front of the object. \n",
    "lights = PointLights(device=device, location=((2.0, 2.0, -2.0),))\n",
    "phong_renderer = MeshRenderer(\n",
    "    rasterizer=MeshRasterizer(\n",
    "        cameras=cameras, \n",
    "        raster_settings=raster_settings\n",
    "    ),\n",
    "    shader=HardPhongShader(device=device, lights=lights, cameras=cameras)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### v2e Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class V2EEmulatorParams: \n",
    "    pos_thres: float = 0.2 #Positive threshold\n",
    "    neg_thres: float = 0.2#Negative Threshold\n",
    "    sigma_thres: float = .03#Sigma Threshold\n",
    "    cutoff_hz: int = 300 #Second-order IIR lowpass filter\n",
    "    leak_rate_hz: float = .01 #per pixel leak event rate in Hz\n",
    "    shot_noise_rate_hz: float = .001 #Noise rate of ON+OFF events\n",
    "    dvs_params: str = None #\"noisy\" or \"clean\" - this will explicitly reset everything else above\n",
    "    output_folder: str = \"data/dump\"\n",
    "    dvs_h5: str = \"test.h5\"#Output as h5 database\n",
    "    dvs_aedat2: str = \"test.aedat\"#Output as AEDAT-2.0 event file\n",
    "    dvs_text: str = \"test.txt\"#Output as txt file\n",
    "\n",
    "@dataclass\n",
    "class V2ERendererParams:\n",
    "    output_path: str = \"data/dump\"\n",
    "    dvs_vid: str = \"dvs-video.avi\"#Output DVS events as AVI video with given frame rate\n",
    "    dvs_vid_full_scale: int = \"2\" #Set full scale event count histogram\n",
    "    preview: bool = False #Disables cv2 preview \n",
    "    exposure_mode: str = \"duration\"#Mode to finish event integration:\n",
    "        #duration s: fixed accumulation in s e.g \"duration .005\"\n",
    "        #count n: Count n events per frame e.g \"count 5000\"\n",
    "        #area_event N M: frame ends when any area of M x N pixels\n",
    "        #fills with N events e.g \"area_count 500 64\"\n",
    "    exposure_value: float = .01 #Corresponds to the value from above\n",
    "    area_dimension: tuple = None#Area dimension from area event"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Emulator Object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from enum import Enum\n",
    "from v2e.renderer import ExposureMode\n",
    "\n",
    "emulator = EventEmulator(\n",
    "    output_folder = \"../data/dump\")\n",
    "emulator.set_dvs_params(\"noisy\")\n",
    "#emulator.show_input = \"diff_frame\"\n",
    "\n",
    "eventRenderer = EventRenderer(\n",
    "    output_path = \"../data/dump\", \n",
    "    dvs_vid = \"test.avi\",\n",
    "    exposure_mode = ExposureMode.COUNT,\n",
    "    exposure_value = 5000,\n",
    "    preview = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Emulate events sequentially"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gray(img):\n",
    "    return np.dot(img[...,:3], [0.2989, 0.5870, 0.1140])\n",
    "\n",
    "def cam_trajectory_rotation(num_points: int = 4):\n",
    "    \"\"\"\n",
    "    Returns: list of camera poses (R,T) from trajectory along a spherical spiral\n",
    "    \"\"\"\n",
    "    \n",
    "    shape = SphericalSpiral(\n",
    "        c = 6,\n",
    "        a = 3,\n",
    "        t_min = 1*math.pi,\n",
    "        t_max=1.05*math.pi,\n",
    "        num_points=num_points)\n",
    "    up = torch.tensor([[1., 0., 0.]])\n",
    "    R = []\n",
    "    T = []\n",
    "    for cp in shape._tuples:\n",
    "        cp = torch.tensor(cp).to(device)\n",
    "        R_new = look_at_rotation(cp[None, :], device=device)\n",
    "        T_new = -torch.bmm(R_new.transpose(1,2), cp[None, :, None])[:, :, 0]\n",
    "        if not len(R) and not len(T):\n",
    "            R = [R_new]\n",
    "            T = [T_new]\n",
    "        else:\n",
    "            R.append(R_new)\n",
    "            T.append(T_new)\n",
    "    return (torch.stack(R)[:,0,:], torch.stack(T)[:,0,:])\n",
    "\n",
    "def cam_trajectory_transform(\n",
    "    dist: list = 2,\n",
    "    elev_range: list= 30.0,\n",
    "    azim_range: list= [0, 360],\n",
    "    batch_size: int = 360):\n",
    "    \"\"\"\n",
    "    Input\n",
    "    Returns cameras from ranges of elevations and azimuths\n",
    "    \n",
    "    Not going to use this for now\n",
    "    \"\"\"\n",
    "    \n",
    "    elev = torch.tensor([elev_range] * batch_size)\n",
    "    idx = 0\n",
    "    pepper = [x / 100. for x in range(-200, 225, 10)]\n",
    "    for e in range(len(elev)):\n",
    "        curr_idx = idx % len(pepper)\n",
    "        elev[e] = elev[e] + pepper[curr_idx]\n",
    "        idx += 1\n",
    "    azim = torch.linspace(azim_range[0], azim_range[1], batch_size)\n",
    "    R, T = look_at_view_transform(dist=dist, elev=elev, azim=azim, device=device)\n",
    "    return (R, T)\n",
    "\n",
    "    \n",
    "def render_trajectory(cam_poses, write_gif=True):\n",
    "    \n",
    "    renders = {\n",
    "        \"phong\": None,\n",
    "        \"silhouette\": None,\n",
    "        \"events\": None\n",
    "    }\n",
    "    events = np.zeros((0,4), dtype = np.float32)\n",
    "    batch_size = 2\n",
    "    #render_manager = RenderManager(\n",
    "    #    types=list(renders.keys()),\n",
    "    #    mesh_name = \"mesh_name\"\n",
    "    #)\n",
    "    # Render the teapot providing the values of R and T.\n",
    "    R, T = cam_poses\n",
    "    for num in range(1, len(R) + 1):\n",
    "        if \"phong\" in renders.keys():\n",
    "            image_ref = phong_renderer(meshes_world=teapot_mesh, R=R[num-1:num:], T=T[num-1:num:])\n",
    "            image_ref = image_ref.cpu().numpy().astype(np.float32)\n",
    "            #plt.subplot(131)\n",
    "            #plt.imshow(image_ref.squeeze())\n",
    "        if \"silhouette\" in renders.keys():\n",
    "            silhouette = silhouette_renderer(meshes_world=teapot_mesh, R=R[num-1:num:], T=T[num-1:num:])\n",
    "            silhouette = silhouette.cpu().numpy()\n",
    "            #plt.subplot(132)\n",
    "            #plt.imshow(silhouette.squeeze()[...,3])  # only plot the alpha channel of the RGBA image \n",
    "        #render_manager.add_images(\n",
    "        #    num,\n",
    "        #    {\"silhouette\": silhouette.squeeze()[...,3], \"phong\": image_ref.squeeze()},\n",
    "        #   R[num-1:num:], T[num-1:num:])\n",
    "        if \"events\" in renders.keys():\n",
    "            img = cv2.cvtColor(image_ref.squeeze(), cv2.COLOR_BGR2GRAY)\n",
    "            img = img.astype(np.float32)\n",
    "            num /= len(R)\n",
    "            newEvents = emulator.generate_events(img, num)\n",
    "            if newEvents is not None and newEvents.shape[0] > 0:\n",
    "                events = np.append(events, newEvents, axis=0)\n",
    "                events = np.array(events)\n",
    "                \n",
    "                event_frames = eventRenderer.render_events_to_frames(events, height=512, width=512, return_frames = True)\n",
    "                for frame in event_frames: \n",
    "                    plt.imshow(frame)\n",
    "                events = np.zeros((0, 4), dtype=np.float32)  # clear array\n",
    "        plt.show()\n",
    "    if len(events)>0: # process leftover\n",
    "        eventRenderer.render_events_to_frames(events, height=512, width=512) \n",
    "        \n",
    "    #render_manager.close()\n",
    "    #return render_manager\n",
    "\n",
    "    \n",
    "cam_poses = cam_trajectory_transform()\n",
    "render = render_trajectory(cam_poses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(events)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch3d",
   "language": "python",
   "name": "pytorch3d"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
