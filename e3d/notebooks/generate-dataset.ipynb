{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Imports and dependencies\n",
    "import os\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0,1\"\n",
    "\n",
    "from os.path import join, abspath, dirname\n",
    "import sys\n",
    "sys.path.insert(0, abspath(join(\"..\", dirname(os.getcwd()))))\n",
    "          \n",
    "import re\n",
    "import random\n",
    "import torch\n",
    "import imageio\n",
    "from PIL import Image, ImageOps\n",
    "from tqdm import tqdm_notebook\n",
    "from skimage import img_as_ubyte\n",
    "from pytorch3d.structures import Meshes\n",
    "from pytorch3d.utils import ico_sphere\n",
    "from pytorch3d.io import load_obj, save_obj\n",
    "from pytorch3d.datasets import (\n",
    "    ShapeNetCore,\n",
    "    collate_batched_meshes\n",
    ")\n",
    "from pytorch3d.loss import (\n",
    "    mesh_laplacian_smoothing, \n",
    "    mesh_normal_consistency\n",
    ")\n",
    "from pytorch3d.renderer import (\n",
    "    PerspectiveCameras, RasterizationSettings, MeshRenderer, MeshRasterizer, \n",
    "    BlendParams, SoftSilhouetteShader, SoftPhongShader, PointLights, TexturesVertex, \n",
    "    TexturesAtlas, HardPhongShader, HardFlatShader, look_at_view_transform\n",
    ")\n",
    "from pytorch3d.io import load_objs_as_meshes\n",
    "\n",
    "from dataclasses import dataclass, field, asdict, astuple\n",
    "\n",
    "import numpy as np\n",
    "#Plotting Libs\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "\n",
    "\n",
    "from synth_dataset.trajectory import cam_trajectory\n",
    "from synth_dataset.mesh import (\n",
    "    load_meshes, mesh_random_translation, rotate_mesh_around_axis, \n",
    "    translate_mesh_on_axis, scale_mesh\n",
    ")\n",
    "from synth_dataset.event_renderer import generate_event_frames\n",
    "\n",
    "from utils.visualization import plot_trajectory_cameras\n",
    "from utils.manager import RenderManager, ImageManager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "mpl.rcParams['savefig.dpi'] = 150\n",
    "mpl.rcParams['figure.dpi'] = 150\n",
    "#Set the device\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "torch.cuda.set_device(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shapenet_path = \"../data/ShapeNetCorev2\"\n",
    "car_dir = '02958343'\n",
    "chair_dir = '02691156'\n",
    "plane_dir = '03001627'\n",
    "synsets = [car_dir, chair_dir, plane_dir]\n",
    "shapenet_dataset = ShapeNetCore(shapenet_path, version=2, synsets=synsets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PARAMS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class RenderParams:\n",
    "    \n",
    "    img_size: int = (280, 280)\n",
    "    sigma_hand: float = .15\n",
    "    \n",
    "    #Size of the dataset\n",
    "    batch_size: int = 360\n",
    "    data_batch_size: int = int(360 / 8) #what we actually save from the mesh batch\n",
    "    mesh_iter: int = 5\n",
    "        \n",
    "    show_frame: bool = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a Renderer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cameras = PerspectiveCameras(device=device)\n",
    "\n",
    "# To blend the 100 faces we set a few parameters which control the opacity and the sharpness of \n",
    "# edges. Refer to blending.py for more details. \n",
    "blend_params = BlendParams(sigma=1e-4, gamma=1e-4)\n",
    "\n",
    "# Define the settings for rasterization and shading. Here we set the output image to be of size\n",
    "# 256x256. To form the blended image we use 100 faces for each pixel. We also set bin_size and max_faces_per_bin to None which ensure that \n",
    "# the faster coarse-to-fine rasterization method is used. Refer to rasterize_meshes.py for \n",
    "# explanations of these parameters. Refer to docs/notes/renderer.md for an explanation of \n",
    "# the difference between naive and coarse-to-fine rasterization. \n",
    "raster_settings = RasterizationSettings(\n",
    "    image_size= RenderParams.img_size[0], \n",
    "    blur_radius=np.log(1. / 1e-4 - 1.) * blend_params.sigma, \n",
    "    faces_per_pixel=100, \n",
    ")\n",
    "\n",
    "# Create a silhouette mesh renderer by composing a rasterizer and a shader. \n",
    "silhouette_renderer = MeshRenderer(\n",
    "    rasterizer=MeshRasterizer(\n",
    "        cameras=cameras, \n",
    "        raster_settings=raster_settings\n",
    "    ),\n",
    "    shader=SoftSilhouetteShader(blend_params=blend_params)\n",
    ")\n",
    "\n",
    "\n",
    "# We will also create a phong renderer. This is simpler and only needs to render one face per pixel.\n",
    "raster_settings = RasterizationSettings(\n",
    "    image_size=RenderParams.img_size[0], \n",
    "    blur_radius=0, \n",
    "    faces_per_pixel=1,\n",
    "    max_faces_per_bin=500000\n",
    ")\n",
    "# We can add a point light in front of the object. \n",
    "#lights = PointLights(device=device, location=((2., 2.0, 2.0),))\n",
    "lights = PointLights(\n",
    "    device=device, \n",
    "    location=[[3.0, 3.0, 0.0]], \n",
    "    diffuse_color=((0.3, 0.3, 0.3),),\n",
    "    specular_color=((0.3, 0.3, 0.3),),\n",
    ")\n",
    "\n",
    "phong_renderer = MeshRenderer(\n",
    "    rasterizer=MeshRasterizer(\n",
    "        cameras=cameras, \n",
    "        raster_settings=raster_settings\n",
    "    ),\n",
    "    shader=HardFlatShader(device=device, lights=lights, cameras=cameras)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load obj file\n",
    "path = \"../data/meshes/dolphin/dolphin.obj\"\n",
    "mesh = load_objs_as_meshes([path], create_texture_atlas=False,load_textures=True , device=device)\n",
    "#mesh.textures = TexturesVertex(\n",
    "#                verts_features=torch.ones_like(mesh.verts_padded(), device=device)\n",
    "#            )\n",
    "#verts, faces, _ = load_obj(path)\n",
    "#verts_rgb = torch.zeros_like(verts)\n",
    "#textures = TexturesVertex(verts_features=[verts_rgb.to(device)])\n",
    "#mesh = Meshes([verts], [faces.verts_idx], textures)\n",
    "#mesh = mesh.to(device)\n",
    "\n",
    "verts = mesh.verts_packed()\n",
    "N = verts.shape[0]\n",
    "center = verts.mean(0)\n",
    "scale = max((verts - center).abs().max(0)[0])\n",
    "mesh.offset_verts_(-center.expand(N, 3))\n",
    "mesh.scale_verts_((1.0 / float(scale)));\n",
    "\n",
    "\n",
    "mesh = rotate_mesh_around_axis(mesh, [110,90,180], phong_renderer,dist=2, device=device)\n",
    "#mesh = translate_mesh_on_axis(mesh, [0,-20,-50], phong_renderer, dist=5)\n",
    "#verts, faces = mesh.get_mesh_verts_faces(0)\n",
    "#save_obj(\"../data/meshes/plane_WWII/plane_WWII.obj\", verts, faces)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_with_background(image, background = None, show: bool = False):\n",
    "\n",
    "    def background_generator():\n",
    "            files = []\n",
    "            background_folder = \"../data/sun360\"\n",
    "            files = os.listdir(background_folder)\n",
    "            rand_file = random.randint(0, len(files) - 1)\n",
    "            rand_file_path = abspath(join(background_folder, files[rand_file]))\n",
    "            rand_file_path = abspath(join(background_folder, 'pano_530c02959a7fdf8fdda4bac494ba3724'))\n",
    "            files = os.listdir(rand_file_path)\n",
    "            for img_num in range(len(files)):\n",
    "                path_img = join(rand_file_path, f\"{img_num}.jpg\")\n",
    "                img = Image.open(path_img).resize(RenderParams.img_size)\n",
    "                yield np.array(img).astype(np.uint8)\n",
    "    \n",
    "    if background is None:\n",
    "        background = background_generator()\n",
    "\n",
    "    #Image.fromarray((np.array(image) * 255).astype(np.uint8)).save(\"test.png\")\n",
    "    #image = Image.open(\"test.png\")\n",
    "    \n",
    "    image = img_as_ubyte(np.clip(image,0, 1))[...,:3]\n",
    "    \n",
    "    image = np.array(image).astype(np.uint8)\n",
    "    \n",
    "    try:\n",
    "        image_bg = next(background)\n",
    "    except StopIteration:\n",
    "        background = background_generator()\n",
    "        image_bg = next(background)\n",
    "\n",
    "    #image_thresh = (image > 1) * 255\n",
    "\n",
    "    image_white = np.all(image==[255,255,255], axis=-1)\n",
    "    image[image_white] = image_bg[image_white]\n",
    "    #img_add = np.amin((image_bg + image), 255)\n",
    "    #img_add[img_add > 255] = 255\n",
    "    \n",
    "    if show:\n",
    "        plt.imshow(image)\n",
    "        plt.show()\n",
    "        \n",
    "    return image, background"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "small_car_list = [\n",
    "    \"a1d85821a0666a4d8dc995728b1ad443\",\n",
    "    \"2861ac374a2ea7f997692eea6221681c\",\n",
    "    \"1b1a7af332f8f154487edd538b3d83f6\",\n",
    "    \"7db6c18d976e52e451553ea674d2701f\",\n",
    "    \"ff564f7ec327ed83391a2a133df993ee\",\n",
    "    \"303bbfd0c5862496ec8ca19d7516cb42\",\n",
    "    \"b2b2f4952e4068d955fe55d6e406ecd4\",\n",
    "    \"82ede85c805bd5a85af609a73d2c2947\",\n",
    "    \"4cabd6d81c0a9e8c6436916a86a90ed7\",\n",
    "    \"a4fc879c642e8fc4a5a4c80d90b70728\",\n",
    "    \"ef8e257ca685d594473f10e6caaeca56\",\n",
    "    \"c1ac2aee4851937c8e30bdcd3135786b\",\n",
    "    \"29e9a4beeaeea1becf71e2e014ff6f\",\n",
    "    \"5ec7fa8170eee943713e820becfd99b\",\n",
    "    \"813bedf2a45f5681ca92a4cdad802b45\",\n",
    "    \"c0b2a4cec94f10436f0bd9fb2f72f93d\",\n",
    "    \"145e18e4ec54ed5792abb3e9ac4cd40c\",\n",
    "    \"99f49d11dad8ee25e517b5f5894c76d9\"\n",
    "]\n",
    "\n",
    "car_exclude = [\n",
    "    \"\"\n",
    "]\n",
    "car_exclude_pytorch3d = ['97d0903cf8912c3ee9d790a68c844819', '3ff887eaebf0bc7e9d2b99af43da16b3', 'e5d6df012b219fa6a7c4dca3ad2d19bf', '6c39e401487c95e9ee897525d11b0599', '397d2e2b3e0988a2d3901a534bc610d8', '4fd5c18c1536d65be129fc90649e41d3', 'f60779c934ee51eddd1e15301c83686f', '350be6825c19fb14e0675251723e1e08', '633dd9755319ce369dfd5136ef0f2af', '97d0903cf8912c3ee9d790a68c844819', '19f52dd4592c3fb5531e940de4b7770d', '9a92ea1009f6b5127b5d9dbd93af5e1a', 'eadebe4328e2c7d7c10520be41d00de2']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ShapeNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = \"models/model_normalized.obj\"\n",
    "car_list = os.listdir(join(shapenet_path, car_dir))\n",
    "print(len(car_list))\n",
    "#car_list = random.sample(small_car_list, k=5)\n",
    "\n",
    "car_paths = [join(shapenet_path, car_dir, c) for c in car_list]\n",
    "car_model_paths = [join(c, model_path) for c in car_paths]\n",
    "meshes = []\n",
    "for num, car in enumerate(car_model_paths):\n",
    "    try:\n",
    "        verts, faces, aux = load_obj(car, load_textures=True, create_texture_atlas=True)\n",
    "        mesh = Meshes(\n",
    "            verts=[verts],\n",
    "            faces=[faces.verts_idx],\n",
    "            textures=TexturesAtlas(atlas=[aux.texture_atlas]),\n",
    "        ).to(device)\n",
    "        meshes.append(mesh)\n",
    "        print(f\"Added mesh: {car_list[num]}\")\n",
    "    except Exception as e:\n",
    "        car_exclude_pytorch3d.append(car_list[num])\n",
    "        print(e, car_list[num])\n",
    "        continue\n",
    "'''\n",
    "mesh = []\n",
    "for car in car_model_paths:\n",
    "    verts, faces, aux = load_obj(car, load_textures=True, create_texture_atlas=True, texture_atlas_size=4)\n",
    "    textures = aux.texture_atlas\n",
    "    if textures is None:\n",
    "        textures = verts.new_ones(\n",
    "            faces.verts_idx.shape[0],\n",
    "            self.texture_resolution,\n",
    "            self.texture_resolution,\n",
    "            3,\n",
    "        )\n",
    "    textures = TexturesAtlas(atlas=textures)\n",
    "    mesh.append(Meshes(verts, faces, textures))\n",
    "'''\n",
    "#Render the models\n",
    "#R, T = look_at_view_transform(dist=5, azim=10, elev=80, device=device)\n",
    "#cameras = PerspectiveCameras(device=device, R=R, T=T)\n",
    "images = []\n",
    "for count, m in enumerate(meshes):\n",
    "    try:\n",
    "        rotate_mesh_around_axis(m, [0,90,0], phong_renderer,dist=.7, device=device)\n",
    "        print(small_car_list[count])\n",
    "    except Exception as e:\n",
    "        car_exclude_pytorch3d.append(car_list[count])\n",
    "        print(e, car_list[count])\n",
    "        continue\n",
    "\"\"\"\n",
    "car_mesh_dicts = [shapenet_dataset[car] for car in range(len(cars))]\n",
    "car_mesh_dicts = collate_batched_meshes(car_mesh_dicts)\n",
    "for mesh in car_mesh_dicts['mesh']:\n",
    "    mesh = mesh.to(device)\n",
    "    image = phong_renderer(meshes_world=mesh.to(device), R=R, T=T, device=device)\n",
    "    plt.imshow(image.squeeze().cpu().numpy()[...,:3])\n",
    "    plt.show()\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Data Creation Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "renders = {\n",
    "    \"phong\": None,\n",
    "    \"silhouette\": None,\n",
    "    \"events\": None\n",
    "}\n",
    "\n",
    "\n",
    "#meshes = load_meshes()\n",
    "# Set paths\n",
    "#DATA_DIR = \"../data/meshes\"\n",
    "#obj_filename = os.path.join(DATA_DIR, \"dog/dog.obj\")\n",
    "\n",
    "# Load obj file\n",
    "#mesh = load_objs_as_meshes([obj_filename], device=device)\n",
    "\n",
    "name = \"dolphin\"\n",
    "mesh_name = \"dolphin\"\n",
    "#Iterate over each mesh\n",
    "#for name, mesh in meshes.items():\n",
    "    \n",
    "\"\"\"Augmentation scenarios (all trajectories complete full 360 w/ simulated handshake)\n",
    "    -normal trajectory\n",
    "    -varying distance\n",
    "    -\n",
    "\"\"\"\n",
    "model_path = \"models/model_normalized.obj\"\n",
    "car_dir = '02958343'\n",
    "chair_dir = '02691156'\n",
    "plane_dir = '03001627'\n",
    "car_list = os.listdir(join(shapenet_path, car_dir))\n",
    "#car_list = random.sample(small_car_list, k=5)\n",
    "\n",
    "car_paths = [join(shapenet_path, car_dir, c) for c in car_list]\n",
    "car_model_paths = [join(c, model_path) for c in car_paths][578:]\n",
    "for num, car_path in enumerate(car_model_paths):\n",
    "    '''\n",
    "    try:\n",
    "        verts, faces, aux = load_obj(car_path, load_textures=True, create_texture_atlas=True)\n",
    "        print(verts.shape)\n",
    "        print(faces.verts_idx.shape)\n",
    "        break\n",
    "        mesh = Meshes(\n",
    "            verts=[verts],\n",
    "            faces=[faces.verts_idx],\n",
    "            textures=TexturesAtlas(atlas=[aux.texture_atlas]),\n",
    "        ).to(device)\n",
    "        print(f\"Adding mesh num {num}: {car_list[num]} \")\n",
    "    except Exception as e:\n",
    "        car_exclude_pytorch3d.append(car_list[num])\n",
    "        print(e, car_list[num])\n",
    "        continue\n",
    "    \n",
    "    \n",
    "    '''\n",
    "    #Create a random trajectory\n",
    "    cam_poses = cam_trajectory(\n",
    "        variation,\n",
    "        pepper,\n",
    "        random_start,\n",
    "        RenderParams.batch_size\n",
    "    )\n",
    "    \n",
    "    mesh, translation = mesh_random_translation(mesh, .1, device=device)\n",
    "    mesh = mesh.to(device)\n",
    "    background = None\n",
    "    \n",
    "    #Batch indices to actually save\n",
    "    data_indices = sorted(random.sample(range(RenderParams.batch_size), k=RenderParams.data_batch_size))\n",
    "    print(data_indices)\n",
    "    \n",
    "    render_manager = RenderManager(\n",
    "        types=list(renders.keys()),\n",
    "        mesh_name = mesh_name,\n",
    "        new_folder = f\"bg_ablation_{name}\",\n",
    "        metadata = {\n",
    "            \"augmentation_params\": {\n",
    "                \"variation\": variation,\n",
    "                \"pepper\": pepper,\n",
    "                \"random_start\": random_start\n",
    "            },\n",
    "            \"mesh_transformation\": {\n",
    "                \"translation\": translation.get_matrix().cpu().numpy().tolist()\n",
    "            },\n",
    "            \"mesh_info\": {\n",
    "                \"mesh_id\":\"\",\n",
    "                \"synset_id\":\"\",\n",
    "                \"category_name\": \"car\"\n",
    "            }\n",
    "        }\n",
    "    )\n",
    "    render_manager.init()\n",
    "    # Render the teapot providing the values of R and T.\n",
    "    R, T = cam_poses\n",
    "    data_img_num = 0\n",
    "    for idx in range(1, len(R) + 1):\n",
    "        img_dict = {}\n",
    "                \n",
    "        if \"phong\" in renders.keys():\n",
    "            #try:\n",
    "            image_ref = phong_renderer(meshes_world=mesh, R=R[idx-1:idx:], T=T[idx-1:idx:], device=device)\n",
    "            #except Exception as e:\n",
    "            #    car_exclude_pytorch3d.append(car_list[num])\n",
    "            #    print(e, car_list[num])\n",
    "            #    continue\n",
    "            image_ref = image_ref.cpu().numpy()\n",
    "            img_dict[\"phong\"] = image_ref.squeeze()\n",
    "            \n",
    "\n",
    "        if \"silhouette\" in renders.keys():\n",
    "            \"\"\"\n",
    "            silhouette = silhouette_renderer(meshes_world=mesh, R=R[num-1:num:], T=T[num-1:num:])\n",
    "            silhouette = silhouette.cpu().numpy()\n",
    "            img_dict[\"silhouette\"] = silhouette.squeeze()[...,3]\n",
    "            \"\"\"\n",
    "            #Creating a mask from the image instead of using the silhouette renderer\n",
    "            silhouette = np.clip(((img_dict[\"phong\"][...,:3]).astype(np.uint8)) * 255, 0 , 255)\n",
    "            silhouette = (silhouette < 1) * 255\n",
    "            img_dict[\"silhouette\"] = silhouette\n",
    "\n",
    "        #Merge with background images\n",
    "        #img, background = merge_with_background(img_dict[\"phong\"], background, show=False)\n",
    "        #img_dict[\"phong\"] = img\n",
    "        if RenderParams.show_frame:\n",
    "            for plot_num, img in enumerate(img_dict.values()):\n",
    "                plot_num += 1\n",
    "                ax = plt.subplot(1, len(img_dict.values()), plot_num)\n",
    "                ax.imshow(img)\n",
    "            plt.show()\n",
    "        \n",
    "        #Only add the image dict if the image was randomly selected\n",
    "        if idx - 1 in data_indices:\n",
    "            #print(num - 1)\n",
    "            render_manager.add_images( \n",
    "                data_img_num,\n",
    "                img_dict,\n",
    "                R[idx-1:idx:], T[idx-1:idx:])\n",
    "            data_img_num+=1\n",
    "        \n",
    "        extra_args = {\"compress_level\": 3}\n",
    "        imageio.imwrite(f'tmp/{idx}.png', img_dict['phong'], format='PNG', **extra_args)\n",
    "        \n",
    "    \n",
    "    image_path_list = []\n",
    "    t = [s for s in os.listdir('tmp') if s.endswith('.png')]\n",
    "    for f in sorted(t, key=lambda s : int(re.sub(r\"\\D\",\"\", s))):\n",
    "        image_path_list.append(join('tmp', f))\n",
    "    \n",
    "    #print(\"image_path_list: \", image_path_list)\n",
    "    event_frames = generate_event_frames(image_path_list, RenderParams.img_size, RenderParams.batch_size)\n",
    "    #print(\"event frames count: \", len(event_frames))\n",
    "    event_count = 0\n",
    "    for ev_count, frame in enumerate(event_frames):\n",
    "        frame = frame * 4\n",
    "        all_white = np.zeros((frame.shape), dtype=np.uint8)\n",
    "        all_white.fill(255)\n",
    "        frame_black = np.all(frame==[0,0,0], axis=-1)\n",
    "        frame[frame_black] = all_white[frame_black]\n",
    "        if ev_count in data_indices:\n",
    "            #print(num)\n",
    "            render_manager.add_event_frame(event_count, frame)\n",
    "            event_count+=1\n",
    "\n",
    "    render_manager.close()\n",
    "    for f in sorted(os.listdir('tmp')):\n",
    "        if f.endswith('.png'):\n",
    "            os.remove(join('tmp', f))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = T[:, 0].cpu()\n",
    "y = T[:, 1].cpu()\n",
    "z = T[:, 2].cpu()\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "ax.scatter3D(x, y, z, marker='o', color='blue')\n",
    "\n",
    "camera = PerspectiveCameras(device=device, R=R, T=T)\n",
    "cam_wvt = cameras.get_world_to_view_transform()\n",
    "cam_center = cameras.get_camera_center()\n",
    "print(cam_center[0])\n",
    "x = cam_center[:, 0].cpu()\n",
    "y = cam_center[:, 1].cpu()\n",
    "z = cam_center[:, 2].cpu()\n",
    "ax.scatter3D(x, y, z, marker='o', color='red')\n",
    "\n",
    "P = cam_wvt.inverse().get_matrix()[0]\n",
    "print(P)\n",
    "t = P[3, :3]\n",
    "r = P[3:, ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting Event Volume"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mpl_toolkits.mplot3d import Axes3D  # noqa: F401 unused import\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "import esim_py\n",
    "\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "@dataclass\n",
    "class EsimParams:\n",
    "\n",
    "    Cp: float = 0.5\n",
    "    Cn: float = 0.5\n",
    "    sigma_cp: float = 0.03\n",
    "    sigma_cn: float = 0.03\n",
    "    refractory_period: float = 1e-4\n",
    "    log_eps: float = 1e-3\n",
    "    use_log: bool = True\n",
    "\n",
    "    show_frame: bool = False\n",
    "\n",
    "render_manager = RenderManager.from_directory(1, 'data/renders/test_car_shapenet')\n",
    "        \n",
    "image_path_list = [img['image_path'] for img in render_manager.images['phong']]\n",
    "\n",
    "timestamp_list = range(len(image_path_list))\n",
    "esim = esim_py.EventSimulator(\n",
    "    EsimParams.Cp,\n",
    "    EsimParams.Cn,\n",
    "    EsimParams.refractory_period,\n",
    "    EsimParams.log_eps,\n",
    "    EsimParams.use_log,\n",
    "    EsimParams.sigma_cp,\n",
    "    EsimParams.sigma_cn,\n",
    ")\n",
    "\n",
    "events = esim.generateFromStampedImageSequence(\n",
    "    image_path_list, timestamp_list\n",
    ")\n",
    "\n",
    "batch_events_plot = int(len(events) / int(360 / 25))\n",
    "print(batch_events_plot)\n",
    "\n",
    "event_batch_size = 0\n",
    "event_frames = []\n",
    "\n",
    "curr_batch_events = events[\n",
    "        event_batch_size : event_batch_size + batch_events_plot\n",
    "    ]\n",
    "\n",
    "fig = plt.figure(figsize=(25,12))\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "y = curr_batch_events[:, 0]\n",
    "z = curr_batch_events[:, 1]\n",
    "x = curr_batch_events[:, 2] * 2 #timestamp \n",
    "m = 'o'\n",
    "c = ['red' if p == 1 else 'blue' for p in curr_batch_events[:, 3]]\n",
    "\n",
    "ax.scatter3D(x, y, z, c=c, marker=m, s=.2)\n",
    "ax.set_xlabel('Time [s]')\n",
    "ax.set_ylabel('x [pix]')\n",
    "ax.set_zlabel('y [pix]')\n",
    "#plt.axis('off')\n",
    "#plt.grid(b=None)\n",
    "#ax.invert_yaxis()\n",
    "plt.show()\n",
    "fig.savefig('ev_volume.png', dpi=fig.dpi)\n",
    "'''\n",
    "pd_dict = pd.DataFrame(dict(x = x, y = y, z = z, c = c))\n",
    "fig = go.Figure()\n",
    "fig = px.scatter_3d(pd_dict, x=\"x\", y=\"y\", z=\"z\")\n",
    "fig.update_layout(title_text=\"Event Volume\",)\n",
    "fig.show()\n",
    "'''\n",
    "img_size = (280, 280)\n",
    "event_batch_size = 0\n",
    "batch_events_images = int(len(events) / 2)\n",
    "\n",
    "while event_batch_size <= batch_events_plot :\n",
    "\n",
    "    curr_batch_events = events[\n",
    "        event_batch_size : event_batch_size + batch_events_images\n",
    "    ]\n",
    "\n",
    "    pos_events = curr_batch_events[curr_batch_events[:, -1] == 1]\n",
    "    neg_events = curr_batch_events[curr_batch_events[:, -1] == -1]\n",
    "\n",
    "    image_pos = np.zeros(img_size[0] * img_size[1], dtype=\"uint8\")\n",
    "    image_neg = np.zeros(img_size[0] * img_size[1], dtype=\"uint8\")\n",
    "\n",
    "    np.add.at(\n",
    "        image_pos,\n",
    "        (pos_events[:, 0] + pos_events[:, 1] * 280).astype(\"int32\"),\n",
    "        pos_events[:, -1] ** 2,\n",
    "    )\n",
    "    np.add.at(\n",
    "        image_neg,\n",
    "        (neg_events[:, 0] + neg_events[:, 1] * 280).astype(\"int32\"),\n",
    "        neg_events[:, -1] ** 2,\n",
    "    )\n",
    "\n",
    "    image_rgb = (\n",
    "        np.stack(\n",
    "            [\n",
    "                image_pos.reshape(img_size),\n",
    "                np.zeros(img_size, dtype=\"uint8\"),\n",
    "                image_neg.reshape(img_size),\n",
    "            ],\n",
    "            -1,\n",
    "        )\n",
    "        * 50\n",
    "    )\n",
    "\n",
    "    # img_black = np.all(image_rgb == [0,0,0], axis=-1)\n",
    "    # image_rgb[img_black] = [255, 255, 255]\n",
    "    #all_white = np.zeros((frame.shape), dtype=np.uint8)\n",
    "    #all_white.fill(255)\n",
    "    #frame_black = np.all(image_rgb==[0,0,0], axis=-1)\n",
    "    #image_rgb[frame_black] = all_white[frame_black]\n",
    "   \n",
    "    #event_frames.append(image_rgb)\n",
    "    \n",
    "\n",
    "    event_batch_size += len(curr_batch_events)\n",
    "\n",
    "\n",
    "z, y = np.ogrid[0:event_frames[0].shape[0], 0:event_frames[1].shape[1]]\n",
    "fig = plt.figure()\n",
    "ax = fig.gca(projection='3d')\n",
    "x = np.zeros_like(y)\n",
    "for i, ev in enumerate(event_frames):\n",
    "    ev = np.pad(ev, pad_width=((1,1), (1,1), (0,0)), constant_values=0, mode='constant') / 255\n",
    "    print(ev.shape)\n",
    "    ax.plot_surface(x + i, y, z, rstride=3, cstride=3, facecolors=np.rot90(ev, 2, (0,1)), shade=False, antialiased=True)\n",
    "plt.axis('off')\n",
    "plt.grid(b=None)\n",
    "plt.show()\n",
    "fig.savefig('stack.png', dpi=fig.dpi)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "r, g, b = event_frames[0][:, :, 0], event_frames[0][:, :, 1], event_frames[0][:, :, 2]\n",
    "r = r.flatten()\n",
    "g = g.flatten()\n",
    "b = b.flatten()\n",
    "'''\n",
    "\n",
    "from matplotlib.image import imread\n",
    "\n",
    "ev = imread('103_phong.png')\n",
    "ev = event_frames[0] / 255\n",
    "print(ev.shape)\n",
    "plt.imshow(ev)\n",
    "plt.show()\n",
    "x, y = np.ogrid[0:ev.shape[0], 0:ev.shape[1]]\n",
    "ax = plt.gca(projection='3d')\n",
    "z = np.zeros_like(y)\n",
    "#ax.plot_surface(x, y, np.ones(ev.shape[:2])*4, rstride=1, cstride=1, facecolors=ev)\n",
    "#ax.plot_surface(x, y, np.ones(ev.shape[:2])*, rstride=1, cstride=1, facecolors=ev)\n",
    "ax.plot_surface(x, y, z, rstride=2, cstride=2, facecolors=ev, shade=False, antialiased=True)\n",
    "plt.show()\n",
    "\n",
    "'''\n",
    "ev = event_frames[1]\n",
    "nr, nc = ev.shape[:2]\n",
    "x,y = np.mgrid[:nr, :nc]\n",
    "z = np.ones((nr, nc))\n",
    "\n",
    "vv.functions.surf(x, y, z, ev, aa=3)\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"../data/renders/test2_dolphin/003-dolphin_2020-10-29T00:30:43\"\n",
    "man = RenderManager.from_path(path)\n",
    "for idx in range(len(man)):\n",
    "    ev_frame = np.array(man.get_event_frame(idx))\n",
    "    all_white = np.zeros((ev_frame.shape), dtype=np.uint8)\n",
    "    all_white.fill(255)\n",
    "    frame_black = np.all(ev_frame==[0,0,0], axis=-1)\n",
    "    ev_frame[frame_black] = all_white[frame_black]\n",
    "    man.add_event_frame(idx, ev_frame)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
