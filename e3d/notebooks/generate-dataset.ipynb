{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Imports and dependencies\n",
    "import os\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0,1\"\n",
    "\n",
    "from os.path import join, abspath, dirname\n",
    "import sys\n",
    "sys.path.insert(0, abspath(join(\"..\", dirname(os.getcwd()))))\n",
    "          \n",
    "import random\n",
    "import torch\n",
    "from PIL import Image, ImageOps\n",
    "from tqdm import tqdm_notebook\n",
    "from skimage import img_as_ubyte\n",
    "from pytorch3d.structures import Meshes\n",
    "from pytorch3d.utils import ico_sphere\n",
    "from pytorch3d.io import load_obj, save_obj\n",
    "from pytorch3d.loss import (\n",
    "    mesh_laplacian_smoothing, \n",
    "    mesh_normal_consistency\n",
    ")\n",
    "from pytorch3d.renderer import (\n",
    "    SfMPerspectiveCameras, RasterizationSettings, MeshRenderer, MeshRasterizer, \n",
    "    BlendParams, SoftSilhouetteShader, SoftPhongShader, PointLights, TexturesVertex, \n",
    "    TexturesAtlas, HardPhongShader, HardFlatShader\n",
    ")\n",
    "from pytorch3d.io import load_objs_as_meshes\n",
    "\n",
    "from dataclasses import dataclass, field, asdict, astuple\n",
    "\n",
    "import numpy as np\n",
    "#Plotting Libs\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "\n",
    "\n",
    "from synth_dataset.trajectory import cam_trajectory\n",
    "from synth_dataset.mesh import (\n",
    "    load_meshes, mesh_random_translation, rotate_mesh_around_axis, \n",
    "    translate_mesh_on_axis, scale_mesh\n",
    ")\n",
    "from synth_dataset.event_renderer import generate_event_frames\n",
    "\n",
    "from utils.visualization import plot_trajectory_cameras\n",
    "from utils.manager import RenderManager, ImageManager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "mpl.rcParams['savefig.dpi'] = 150\n",
    "mpl.rcParams['figure.dpi'] = 150\n",
    "#Set the device\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "torch.cuda.set_device(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PARAMS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class RenderParams:\n",
    "    \n",
    "    img_size: int = (280, 280)\n",
    "    sigma_hand: float = .15\n",
    "    \n",
    "    #Size of the dataset\n",
    "    mini_batch: int = 72\n",
    "    batch_size: int = 360\n",
    "    mesh_iter: int = 12\n",
    "        \n",
    "    show_frame: bool = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a Renderer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cameras = SfMPerspectiveCameras(device=device)\n",
    "\n",
    "# To blend the 100 faces we set a few parameters which control the opacity and the sharpness of \n",
    "# edges. Refer to blending.py for more details. \n",
    "blend_params = BlendParams(sigma=1e-4, gamma=1e-4)\n",
    "\n",
    "# Define the settings for rasterization and shading. Here we set the output image to be of size\n",
    "# 256x256. To form the blended image we use 100 faces for each pixel. We also set bin_size and max_faces_per_bin to None which ensure that \n",
    "# the faster coarse-to-fine rasterization method is used. Refer to rasterize_meshes.py for \n",
    "# explanations of these parameters. Refer to docs/notes/renderer.md for an explanation of \n",
    "# the difference between naive and coarse-to-fine rasterization. \n",
    "raster_settings = RasterizationSettings(\n",
    "    image_size= RenderParams.img_size[0], \n",
    "    blur_radius=np.log(1. / 1e-4 - 1.) * blend_params.sigma, \n",
    "    faces_per_pixel=100, \n",
    ")\n",
    "\n",
    "# Create a silhouette mesh renderer by composing a rasterizer and a shader. \n",
    "silhouette_renderer = MeshRenderer(\n",
    "    rasterizer=MeshRasterizer(\n",
    "        cameras=cameras, \n",
    "        raster_settings=raster_settings\n",
    "    ),\n",
    "    shader=SoftSilhouetteShader(blend_params=blend_params)\n",
    ")\n",
    "\n",
    "\n",
    "# We will also create a phong renderer. This is simpler and only needs to render one face per pixel.\n",
    "raster_settings = RasterizationSettings(\n",
    "    image_size=RenderParams.img_size[0], \n",
    "    blur_radius=1e-5, \n",
    "    faces_per_pixel=100, \n",
    ")\n",
    "# We can add a point light in front of the object. \n",
    "#lights = PointLights(device=device, location=((2., 2.0, 2.0),))\n",
    "lights = PointLights(\n",
    "    device=device, \n",
    "    location=[[3.0, 3.0, 0.0]], \n",
    "    diffuse_color=((1.0, 1.0, 1.0),),\n",
    "    specular_color=((1.0, 1.0, 1.0),),\n",
    ")\n",
    "\n",
    "phong_renderer = MeshRenderer(\n",
    "    rasterizer=MeshRasterizer(\n",
    "        cameras=cameras, \n",
    "        raster_settings=raster_settings\n",
    "    ),\n",
    "    shader=HardFlatShader(device=device, lights=lights, cameras=cameras)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load obj file\n",
    "path = \"../data/meshes/dolphin/dolphin.obj\"\n",
    "mesh = load_objs_as_meshes([path], create_texture_atlas=False,load_textures=True, device=device)\n",
    "#mesh.textures = TexturesVertex(\n",
    "#                verts_features=torch.ones_like(mesh.verts_padded(), device=device)\n",
    "#            )\n",
    "\n",
    "verts = mesh.verts_packed()\n",
    "N = verts.shape[0]\n",
    "center = verts.mean(0)\n",
    "scale = max((verts - center).abs().max(0)[0])\n",
    "mesh.offset_verts_(-center.expand(N, 3))\n",
    "mesh.scale_verts_((1.0 / float(scale)));\n",
    "\n",
    "\n",
    "mesh = rotate_mesh_around_axis(mesh, [90,90,180], phong_renderer,dist=2, device=device)\n",
    "#mesh = translate_mesh_on_axis(mesh, [0,-20,-50], phong_renderer, dist=5)\n",
    "#verts, faces = mesh.get_mesh_verts_faces(0)\n",
    "#save_obj(\"../data/meshes/plane_WWII/plane_WWII.obj\", verts, faces)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_with_background(image, background = None, show: bool = False):\n",
    "\n",
    "    def background_generator():\n",
    "            files = []\n",
    "            background_folder = \"../data/sun360\"\n",
    "            files = os.listdir(background_folder)\n",
    "            rand_file = random.randint(0, len(files) - 1)\n",
    "            rand_file_path = abspath(join(background_folder, files[rand_file]))\n",
    "            files = os.listdir(rand_file_path)\n",
    "            for img_num in range(len(files)):\n",
    "                path_img = join(rand_file_path, f\"{img_num}.jpg\")\n",
    "                img = Image.open(path_img).resize(RenderParams.img_size)\n",
    "                yield np.array(img).astype(np.uint8)\n",
    "    \n",
    "    if background is None:\n",
    "        background = background_generator()\n",
    "\n",
    "    #Image.fromarray((np.array(image) * 255).astype(np.uint8)).save(\"test.png\")\n",
    "    #image = Image.open(\"test.png\")\n",
    "    \n",
    "    image = img_as_ubyte(np.clip(image,0, 1))[...,:3]\n",
    "    \n",
    "    image = np.array(image).astype(np.uint8)\n",
    "    \n",
    "    try:\n",
    "        image_bg = next(background)\n",
    "    except StopIteration:\n",
    "        background = background_generator()\n",
    "        image_bg = next(background)\n",
    "\n",
    "    #image_thresh = (image > 1) * 255\n",
    "\n",
    "    image_white = np.all(image==[255,255,255], axis=-1)\n",
    "    image[image_white] = image_bg[image_white]\n",
    "    #img_add = np.amin((image_bg + image), 255)\n",
    "    #img_add[img_add > 255] = 255\n",
    "    \n",
    "    if show:\n",
    "        plt.imshow(image)\n",
    "        plt.show()\n",
    "        \n",
    "    return image, background"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Data Creation Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "renders = {\n",
    "    \"phong\": None,\n",
    "    \"silhouette\": None,\n",
    "    \"events\": None\n",
    "}\n",
    "variation = [\"dist\", \"elev\"]\n",
    "pepper = [\"elev\"]\n",
    "random_start = [\"azim\"]\n",
    "\n",
    "#meshes = load_meshes()\n",
    "# Set paths\n",
    "#DATA_DIR = \"../data/meshes\"\n",
    "#obj_filename = os.path.join(DATA_DIR, \"dog/dog.obj\")\n",
    "\n",
    "# Load obj file\n",
    "#mesh = load_objs_as_meshes([obj_filename], device=device)\n",
    "\n",
    "name = \"dolphin\"\n",
    "mesh_name = \"dolphin\"\n",
    "#Iterate over each mesh\n",
    "#for name, mesh in meshes.items():\n",
    "    \n",
    "\"\"\"Augmentation scenarios (all trajectories complete full 360 w/ simulated handshake)\n",
    "    -normal trajectory\n",
    "    -varying distance\n",
    "    -\n",
    "\"\"\"\n",
    "count = 0\n",
    "while count != RenderParams.mesh_iter:\n",
    "\n",
    "    #Create a random trajectory\n",
    "    cam_poses = cam_trajectory(\n",
    "        variation,\n",
    "        pepper,\n",
    "        random_start,\n",
    "        RenderParams.batch_size\n",
    "    )\n",
    "\n",
    "    mesh, translation = mesh_random_translation(mesh, .0, device=device)\n",
    "\n",
    "    background = None\n",
    "\n",
    "    render_manager = RenderManager(\n",
    "        types=list(renders.keys()),\n",
    "        mesh_name = mesh_name,\n",
    "        new_folder = f\"train2_{name}\",\n",
    "        metadata = {\n",
    "            \"augmentation_params\": {\n",
    "                \"variation\": variation,\n",
    "                \"pepper\": pepper,\n",
    "                \"random_start\": random_start\n",
    "            },\n",
    "            \"mesh_transformation\": {\n",
    "                \"translation\": translation.get_matrix().cpu().numpy().tolist()\n",
    "            }\n",
    "        }\n",
    "    )\n",
    "    render_manager.init()\n",
    "    # Render the teapot providing the values of R and T.\n",
    "    R, T = cam_poses\n",
    "    for num in range(1, len(R) + 1):\n",
    "        img_dict = {}\n",
    "\n",
    "        if \"phong\" in renders.keys():\n",
    "            image_ref = phong_renderer(meshes_world=mesh, R=R[num-1:num:], T=T[num-1:num:])\n",
    "            image_ref = image_ref.cpu().numpy()\n",
    "            img_dict[\"phong\"] = image_ref.squeeze()\n",
    "            \n",
    "\n",
    "        if \"silhouette\" in renders.keys():\n",
    "            \"\"\"\n",
    "            silhouette = silhouette_renderer(meshes_world=mesh, R=R[num-1:num:], T=T[num-1:num:])\n",
    "            silhouette = silhouette.cpu().numpy()\n",
    "            img_dict[\"silhouette\"] = silhouette.squeeze()[...,3]\n",
    "            \"\"\"\n",
    "            #Creating a mask from the image instead of using the silhouette renderer\n",
    "            silhouette = np.clip(((img_dict[\"phong\"][...,:3]).astype(np.uint8)) * 255, 0 , 255)\n",
    "            silhouette = (silhouette < 1) * 255\n",
    "            img_dict[\"silhouette\"] = silhouette\n",
    "\n",
    "        #Merge with background images\n",
    "        img, background = merge_with_background(img_dict[\"phong\"], background, show=False)\n",
    "        img_dict[\"phong\"] = img\n",
    "        if RenderParams.show_frame:\n",
    "            for plot_num, img in enumerate(img_dict.values()):\n",
    "                plot_num += 1\n",
    "                ax = plt.subplot(1, len(img_dict.values()), plot_num)\n",
    "                ax.imshow(img)\n",
    "            plt.show()\n",
    "\n",
    "        render_manager.add_images( \n",
    "            num,\n",
    "            img_dict,\n",
    "            R[num-1:num:], T[num-1:num:])\n",
    "\n",
    "    image_path_list = [img['image_path'] for img in render_manager.images['phong']]\n",
    "    event_frames = generate_event_frames(image_path_list, RenderParams.img_size, RenderParams.batch_size)\n",
    "    for num, frame in enumerate(event_frames):\n",
    "        render_manager.add_event_frame(num, frame)\n",
    "\n",
    "    render_manager.close()\n",
    "    count += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
